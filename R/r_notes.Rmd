# ----- R Notes -----
Sources:
1. [Mastering Health Science Using R](https://alicepaul.github.io/health-data-science-using-r/book.html)

*Keyboard shortcuts:*
- `<-` (assignment operator)
  - Windows: Alt + -
  - Mac: Option + -
- `%>%` (pipe operator)
  - Windows: Ctrl+Shift+M 
  - Mac: Cmd+Shift+M 
- (Un)Comment Selection
  - Windows: Ctrl+Shift+C 
  - Mac: Cmd+Shift+C 
___

## ----- Basics of R -----

### ----- Basic Computations Objects -----
- Addition: `+`
- Subtraction: `-`
- Multiplication: `*`
- Division: `/`
- Exponentiation: `^`
- Modulo: `%%`
```{r}
5 + 6 # addition
7 - 2 # subtraction
2 * 3 # multiplication
6 / 3 # division
4^2 # exponentiation
100 %% 4 # modulo
```
___
- **objects** are values used in R 
  - a **variable** is an object
  - an object is a named instance of a data structure
  - data structures in R are **vectors**, **factors**, **matrices**, **arrays**, **lists**, and **data frames**
- to store any computation or value as a variable you would use the assignment operator:
```{r}
x <- 2 + 3
x <- x + 1
x
```

- calling a function is: `function_name()`
  - examples of functions:
    - `ceiling()` returns the ceiling (an upper usually prescribed limit) of your input
    - `floor()` returns the floor (a lower usually prescribed limit) of your input 
    - `round()` rounds your input to the closest integer - it rounds a number in 0.5 to the closest even integer:
      - ex: 2.5 = 2; 3.5 = 4
```{r}
ceiling(3.7)
floor(3.7)
round(2.5)
round(3.5)
```
___
- to see what the current working directory is you would use `getwd()`
  - to change the working directory, you would use `setwd()`
```{r}
getwd() # "/Users/carolinesanicola/" on Mac
# to set the current working directory, you can use setwd()
setwd("/Users/carolinesanicola/Documents/GitHub/important-reference-repo/R")
# check
getwd()
```
___
- before you do anything else, you need to install packages that are used in R 
  - the most important one is `tidyverse`
    - `install.packages('tidyverse')`
      - to call the package: `library(tidyverse)`
```{r}
# now update the path to the csv file
df <- read.csv("https://raw.githubusercontent.com/alicepaul/health-data-science-using-r/b71bbec95709d0107c166d97654890b9500678ef/book/data/fake_names.csv")
df
# another way to load in a csv is using readr::
df <- readr::read_csv("https://raw.githubusercontent.com/alicepaul/health-data-science-using-r/refs/heads/main/book/data/test.csv", show_col_types = FALSE)

# the most important thing to do is download packages and one of the most important is 'tidyverse'
# to see what packages are already installed use `installed.packages()`
installed.packages()
# the most important package to download is tidyverse
install.packages("tidyverse")
library(tidyverse)
```

___
## ----- Data Structures in R -----
```{r}
# an object is a named instance of a data structure
ex_num <- 4
```
- each individual value in R has a type: **logical**, **integer**, **double** or **character**
  - you can use `typeof()` to find the type of vector 
```{r}
typeof(ex_num)
```
- **double** is a numeric value with a stored decimal
- an **integer** is a whole number without a decimal
  - to indicate we want the number to be an integer object, you need to use `L` after the number 
- **characters** have letters
- **logicals** or **booleans** are `TRUE` and `FALSE`
  - **booleans** can actually be interpreted as `0`/`1` and can be used in logic: ex: `TRUE + FALSE + TRUE` = 2
```{r}
ex_int <- 4L
typeof(ex_int)
# characters have letters
ex_char <- "Alice"
typeof(ex_char)
# logicals or booleans are TRUE and FALSE
ex_bool <- TRUE
typeof(ex_bool)
# booleans can actually be interpreted as 0/1 and can be used in logic
TRUE + FALSE + TRUE
```

### ----- Vectors -----
- Vectors are one-dimensional data structures that cna store multiple data types of the same type (ex: character, Boolean, numeric)
- You can confirm if something is a vector by using: `is.vector()`
```{r}
is.vector(ex_bool)
```
- Creating a vector:
  - combine multiple values using the `c()` function 
```{r}
days <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")
rain <- c(5, 0.1, 0, 0, 0.4)
```
    - vectors can only store the same type of value, so if you try and combine a string and a integer, it will convert the integer to a string 
```{r}
c("Monday", 5)
```
    - the `class()` function returns the data structure of an object 
```{r}
class(days)
class(rain)
```
    - you can create an empty vectio by leaving the `c()` blank
```{r}
ex_empty <- c()
class(ex_empty)
```
      - you can also set the type of the empty vector:
```{r}
ex_empty <- vector(mode = "numeric")
class(ex_empty)
```
  - you can create a vector with `req()` or `seq()`
    - `req(x, times)` takes the value (x) and a number of times and outputs x the number of times entered
```{r}
rep(0, 5)
rep("Monday", 4)
```
    - `seq(from, to step)` takes a numeric starting value (from) and an end value (to) and step size (step) which specifies the sequence to get from the **from** to the **to** value (or the maximum that can get close based on the step value)
```{r}
seq(1, 5, 1)
seq(0, -10, -2)
```

#### ----- Indexing a vector -----
- when a vector is created, you can access certain values in it by indexing []
```{r}
days[1]
days[4]
```
- not only can you access a single value, but a subset of the values using `c()`
```{r}
days[c(1, 4)]
days[-c(1, 4)] # this is saying to return everything BUT the 1 and 4 values
```
- another option is to associate a name with each value
```{r}
names(rain) <- days
print(rain)
rain["Friday"]
```
- another option is using `TRUE` and `FALSE` values to index the values in a vector (both vectors need to be the same length)
  - so when you index the vector, it will only show the `TRUE` values
```{r}
ind_bools <- c(TRUE, FALSE, FALSE, TRUE, FALSE)
days[ind_bools] # this is now going to return only the TRUE values in the vector
```

#### ----- Modifying a Vector and Calculations -----
- exp() - exponential
- log() - log
- sqrt() - square root
- abs() - absolute value
- round() - round to nearest integer value
- ceiling() - round up to the nearest integer value
- floor() - round down to the nearest integer value
- signif(, dig) - round to dig number of significant digits
- the above logical functions can be used to combine vectors or as operators on vector values
```{r}
c(1, 2, 3) + c(1, 1, 1)
c(1, 2, 3) + 1 # the above is identical to this
sqrt(c(1, 4, 16))
signif(c(0.23, 0.19), dig = 1)
```
- you can also change values in a vector: 
```{r}
rain["Friday"] <- 0.5
rain
```
- adding additional entries to a vector is possible too 
  - `days <- c(days, "Saturday", "Sunday")`
  - to check the current length of a vector, you can use the `length()` function 
```{r}
length(rain)
days <- c(days, "Saturday", "Sunday") # add the weekend with no rain
rain <- c(rain, 0, 0)
length(rain)
```
- you can also get the sum, max, and min of a vector using the functions:
```{r}
sum(rain)
max(rain)
min(rain)
```

#### ----- Common Vector Functions -----
- below are some of the most common vector functions available in R 
- sum() - summation
- median() - median value
- mean() - mean
- sd() - standard deviation
- var() - variance
- max() - maximum value
- which.max() - index of the first element with the maximum value
- min() - minimum value
- which.min() - index of the first element with the minimum value
```{r}
mean(rain)
min(rain)
which.min(rain) # this will tell you which value is the min value (if it was given a name)
```
- you can also use the `sort()` function to sort the values
- and the `order()` function to see the order of the values which have the smallest and highest values
- for both `order()` and `sort()`, they have the extra argument of `decreasing` which can be set to either `TRUE` or `FALSE`
```{r}
order(rain) # this is returning what is the lowest values to highest values but in the order they currently are in
days[order(rain)] # this is returning the days of the week in the order of the smallest rain value to largest rain value
days[order(rain, decreasing = TRUE)] # now it will show the days in order of highest rain to lowest rain values
```
### ----- Factors -----
- a special kind of vector that behaves like a regular vector, except that it represents values from a category
- it keeps track of all possible values of that category in what are called levels of the factor 
- the function `as.factor()` converts a vector to a factor
  - using the `factor()` function instead of `as.factor()` will let you specify the levels of the factor even if there are more levels that current values of the factor 
```{r}
days <- c("Monday", "Tuesday", "Wednesday", "Monday", "Thursday", "Wednesday")
days_fct <- as.factor(days)
class(days_fct)
levels(days_fct)

days_fct <- factor(days,
  levels = c(
    "Monday", "Tuesday", "Wednesday",
    "Thursday", "Friday", "Saturday", "Sunday"
  )
)

class(days_fct)
levels(days_fct)
days_fct[2] <- "Friday" # this is going to insert "Friday" into the second index of days_fct
days_fct
```
- you may not have all the values in the factor as what you are specifying in the levels but thats just telling when a new value is inserted, what level the new value will take
- factors can also be used for numeric vectors 
- if we want to have 0/1 to represent whether a day is a weekend or not, this is something we can do with factor levels
```{r}
weekend <- as.factor(c(1, 0, 0, 0, 1, 1))
levels(weekend)
```

### ----- Matrices -----
- similar to vectors in that they store data of the same type but matrices are two-dimensional and consist of rows and columns 
- matrices can be created using the matric() function
  - `matrix(data, nrow, ncol, byrow)`
    - **data** are the values (and are often used with the `c()` function)
    - **nrow** is the number of rows
    - **ncol** is the number of columns
    - **byrow** is either `TRUE` if you want the data values to be filled in by row or `FALSE` if its by columns
- to find the dimensions of the matrix, you can use `nrow()`, `ncol()`, or `dim()`
  - `nrow()` returns how many rows there are 
  - `ncol()` returns how many columns there are 
  - `dim()` returns how many rows and columns there are
```{r}
rainfall <- matrix(
  c(
    5, 6, 0.1, 3, 0, 1, 0, 1, 0.4, 0.2,
    0.5, 0.3, 0, 0
  ),
  ncol = 7, nrow = 2, byrow = TRUE
)
rainfall

nrow(rainfall)
ncol(rainfall)
dim(rainfall)
```

#### ----- Indexing a Matrix -----
- as opposed to vectors, since there are rows and columns, you need to know both when trying to index a value in a matrix
```{r}
rainfall[1, 4]
```
- you can provide multiple indexes to return multiple values
```{r}
rainfall[1, c(4, 5, 7)]
```
- you can also index by boolean
```{r}
rainfall[c(FALSE, TRUE), ] # don't have to include column because we are indexing all the values
rainfall[, c(TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE)]
```
- we can also specify row and column names in the indices
- `colnames()` and `rownames()` returns the names of the rows and columns
```{r}
colnames(rainfall) <- c(
  "Monday", "Tuesday", "Wednesday", "Thursday",
  "Friday", "Saturday", "Sunday"
)
rownames(rainfall) <- c("Week1", "Week2")
rainfall["Week1", c("Friday", "Saturday")]
```
#### ----- Modifying a Matrix -----
- to change the values in a matrix, you need to index the values first and then assign new values
```{r}
rainfall["Week1", "Friday"] <- 3
rainfall["Week1", c("Monday", "Tuesday")] <- 0
print(rainfall)
```
- the functions `rbind()` and `cbind()` can add rows and columns 
```{r}
rainfall <- rbind(rainfall, "Week3" = c(0.4, 0.0, 0.0, 0.0, 1.2, 2.2, 0.0)) # this will add a new row with values
rainfall <- cbind(rainfall, "Total" = c(7.1, 2.4, 3.8)) # this will add a new column with values
print(rainfall)
```
- you can bind matrices together using `cbind()`, but you need to make sure they are the same size
```{r}
A <- matrix(c(1, 2, 3, 4), nrow = 2)
B <- matrix(c(5, 6, 7, 8), nrow = 2)
C <- cbind(A, B)
C
```
- you can also use mathematical operators to combine too: `A+B` or perform mathematics on the matrix: `exp(C)`
```{r}
A + B
exp(C)
```
### ----- Data Frames -----
- like matrices, they store data in rows and columns, but you can use different data types
- you can create a data frame from data using the `data.frame()` function
```{r}
weather_data <- data.frame(
  day_of_week = c("Monday", "Tuesday", "Wednesday", "Monday"),
  temp = c(70, 62, 75, 50),
  rain = c(5, 0.1, 0.0, 0.5)
)
```
  - the `head()` function prints the first six rows of a data frame
```{r}
head(weather_data)
```
  - the `tail()` function prints the last six rows of a data frame
```{r}
tail(weather_data)
```
- you can use the same `dim()`, `nrow()`, `ncol()` as you would with a matrix 
```{r}
dim(weather_data)
nrow(weather_data)
ncol(weather_data)
```
- the same with `colnames()` or you can even just use `names()`
```{r}
colnames(weather_data)
names(weather_data)
rownames(weather_data) <- c("6/1", "6/2", "6/3", "6/8")
head(weather_data)
rownames(weather_data)
```

#### ----- Indexing a Data Frame -----
- done the same way as with a matrix 
```{r}
weather_data[1, 2]
weather_data[1, c("day_of_week", "temp")]
```
- you can also use `$` for accessing the column names:
```{r}
weather_data$day_of_week
weather_data$temp
# the day_of_week column is categorical and can only take a limited number of values so it would be useful to convert it to a factor to limit what is allowed in it
weather_data$day_of_week <- factor(weather_data$day_of_week)
levels(weather_data$day_of_week)
```

#### ----- Modifying a Data Frame -----
- can also modify the data the same as with a matrix 
```{r}
weather_data[1, "rain"] <- 2.2
weather_data
```
- can also combine data frames using the `rbind()` or `cbind()` but can also use `$`
```{r}
weather_data$aq_warning <- as.factor(c(1, 0, 0, 0))
weather_data
```

### ----- Lists -----
- a data frame is actually a special type of another data structure called a list
- a list is a collection of objects under the same name
- these objects can be vectors, matrices, data frames, or even other lists
- they don't have to be the same size, type, or any other matching attribute
- when printing indexes, it will return the sublist of values you are indexing
- you can specify to return just the values themselves by using double brackets: `ex_list[[2]]`
- you can also just name of the elements of a list to make indexing and manipulating easier:
```{r} 
ex_list <- list(
  name = "John",
  medication = c("ibuprofen", "metformin"),
  past_weights = c(136, 142, 159)
)
print(ex_list)
print(class(ex_list[2])) # this will print "list"
ex_list[2] # this is going to return a list of the sublist we are indexing
# to return just the values themselves, you would use double brackets
ex_list[[2]]
# it is easier to name the elements of a list to make accessing easier
ex_list <- list(
  name = "John",
  medication = c("ibuprofen", "metformin"),
  past_weights = c(136, 142, 159)
)
print(ex_list)
# now we have names to easily access things in the list to change or call upon them
ex_list$supplements <- c("vitamin D", "biotin")
ex_list$supplements[2] <- "collagen"
ex_list
```
___
## ----- Working with Data Files in R -----

- Tab-Delimited Files: You can read in a tab-separated .txt file using the read.delim() function in base R.
- Excel Files: You can read in a .xls or .xlsx file using readxl::read_excel(), which allows you to specify a sheet and/or cell range within a file (e.g., read_excel('test.xlsx', sheet="Sheet1")).
- SAS: haven::read_sas() reads in .sas7bdat or .sas7bcat files, haven::read_xpt() reads in SAS transport files.
- Stata: haven::read_dta() reads in .dta files.
- SPSS: haven::read_spss() reads in .spss files.
```{r}
# the library to load in for this is HDSinRdata which contains a sample dataset
install.packages("HDSinRdata")
library(HDSinRdata)
# importing data you would use the data() function
data(pain)
dim(pain)

# another way to use your own data, you would need to install the readr package
install.packages("readr")
library(readr)
# this contains read_csv() function which will be the function most used

# but you can also create a csv file from a dataframe you've created
df <- data.frame(x = c(1, 0, 1), y = c("A", "B", "C"))
# write.csv(df, "data/test.csv", row.names=FALSE)
head(pain)
```
```{r}
# to explore distributions and ranges of the values in certain columns, you can use the summary() function
summary(pain$PAIN_INTENSITY_AVERAGE)
# if there are NA values in the column, you can add `na.rm=TRUE` so that it won't return NA for anything you ask
min(pain$PAIN_INTENSITY_AVERAGE, na.rm = TRUE) # returns the minimum value for a numeric vector
max(pain$PAIN_INTENSITY_AVERAGE, na.rm = TRUE) # returns the maximum value for a numeric vector
mean(pain$PAIN_INTENSITY_AVERAGE, na.rm = TRUE) # returns the mean for a numeric vector
median(pain$PAIN_INTENSITY_AVERAGE, na.rm = TRUE) # returns the median for a numeric vector
range(pain$PAIN_INTENSITY_AVERAGE, na.rm = TRUE) # returns the minimum and maximum values for a numeric vector x
quantile(pain$PAIN_INTENSITY_AVERAGE, na.rm = TRUE) # returns the sample quantiles for a numeric vector
IQR(pain$PAIN_INTENSITY_AVERAGE, na.rm = TRUE) # returns the interquartile range for a numeric vector
# by default the quantile() function returns the sample quantiles but you can specify which quantiles you want to use
quantile(pain$PAIN_INTENSITY_AVERAGE, probs = c(0.1, 0.9), na.rm = TRUE)
```

- you can also see the distribution of values in a numeric column by using the `hist()` function
```{r}
hist(pain$PAIN_INTENSITY_AVERAGE)
```
- the `summary()` function works with categorical columns too but will just return the counts for each level
```{r}
summary(pain$PAT_SEX)
```
  - it is easier to see the output by using the `table()` function: 
```{r}
table(pain$PAT_SEX, useNA = "always") # the useNA argument will show if there are any NA values in the table because by default it ignores NA's in the output
```
- using the `prob.table()` function will convert the counts to proportions (what percentage of the total values is the categorical value)
```{r}
prop.table(table(pain$PAT_SEX))
# because this has a limited types of values that can be in this column, we can convert the column to a factor
is.factor(pain$PAT_SEX)
pain$PAT_SEX <- as.factor(pain$PAT_SEX)
is.factor(pain$PAT_SEX)
```
- you can also take a subset of the data and store it into a new dataframe
```{r}
body_map_cols <- names(pain)[2:75]
pain_body_map <- pain[, body_map_cols]
head(pain_body_map)
hist(rowSums(pain_body_map))
```
```{r}
# we will now fine the number of patients who selected each body region divided by the total number of patients
# this will show the percentage of body regions by total patients
perc_patients <- colSums(pain_body_map, na.rm = TRUE) /
  nrow(pain_body_map)
hist(perc_patients)
```
- the `pmax()` function is to find the maximum value between two columns
```{r}
which.max(perc_patients) # returns which value is the max value
# pmax() and pmin() return the pairwise max or min across the vectors
v1 <- c(5, 9, 12)
v2 <- c(2, 18, 4)
pmax(v1, v2)
# if we want to create a new column called lower_back_pain that corresponds to whether someone selects either x218 or x219, you can use the pmax() function to find the maximum value between columns x218 and x219
lower_back <- pmax(pain_body_map$X218, pain_body_map$X219)
prop.table(table(lower_back))

# if we want to store the total number of pain regions and out indicator of whether or not a patient has a lower back pain as new columns
pain$NUM_REGIONS <- rowSums(pain_body_map)
pain$LOWER_BACK <- lower_back
dim(pain)
```

- another useful function that allows us to perform computations over the rows or columns of a matrix or data frame is the `apply(X, MARGIN, FUN)` function
  - the `X` is the data frame or matrix
  - the `MARGIN` indicates whether to compute over the rows (1) or columns(2)
  - the `FUN` argument is the function to apply across that MARGIN
```{r}
any_selected <- apply(pain_body_map, 1, max)
min(any_selected, na.rm = TRUE)
# the min shows that every patient selected at least one body map region
```
- you can also use `colSums()` to find the sum of the columns
```{r}
# if we use apple() function it will pass any additional arguments to the function FUN
perc_patients <- apply(pain_body_map, 2, sum, na.rm = TRUE) /
  nrow(pain_body_map)
summary(perc_patients)
```

### ----- Missing Infinite and NaN Values -----
- missing values in R are `NA`
- to determine if there is a missing value, you can use the function `is.na()`
  - if you combine the `is.na()` function with `sum()`, you can see how many columns/rows have NA values:
```{r}
sum(is.na(pain$PATIENT_NUM))
# to see if there are NAs accross multiple columns and not just one, we can use apply() function too
num_missing_col <- apply(pain, 2, function(x) sum(is.na(x)))
min(num_missing_col)
```
```{r}
# the above will tell us that there is at least one missing value in each column
# since that is just seeing minimally how many missing, we may want to see if there is an entire column of missing values; to do so, we need to see how many columns there are total and if there is a row that matches that number indicating that there are all missing values for every column for an entire row
num_missing_row <- apply(pain, 1, function(x) sum(is.na(x)))
max(num_missing_row)
which.max(num_missing_row) # this is asking which row has lal the missing values in it
pain <- pain[-11749, ] # this is going to remove that row from the dataset
# once we remove that row, if we find the percentage of missing values per column we can see which has the most NAs
num_missing_col <- apply(pain, 2, function(x) sum(is.na(x)) / nrow(pain))
num_missing_col
# you will see that for PAIN_INTESITY_AVERAGE.FOLLOW_UP it is 0.670422015 which is saying there is only about 33% of responses/valid data for this column
# from this we can create new columns of data
# first is a column for the change in pain at follow-up
pain$PAIN_CHANGE <- pain$PAIN_INTENSITY_AVERAGE.FOLLOW_UP - pain$PAIN_INTENSITY_AVERAGE
hist(pain$PAIN_CHANGE)
# second is a column for the percent change in pain at follow-up
pain$PERC_PAIN_CHANGE <- pain$PAIN_CHANGE / pain$PAIN_INTENSITY_AVERAGE
summary(pain$PERC_PAIN_CHANGE)
# the Max will say 'Inf' which means infinity
```
- to see if something is infinite or not, use the `is.infinite()` or `is.finite()` functions 
```{r}
sum(is.infinite(pain$PERC_PAIN_CHANGE)) # this will test how many rows return infinite values for this column
```
- R also uses `NaN` (aka Not a Number) to show if there is a NaN value such as 0/0
  - you can use the function `is.nan()` to check
- if you want to remove all the na values in a dataframe, you can use the function `na.omit()` 
  - if you want to see how many rows have full values (aka there are no na/nan values in the row), you can use the function `complete.cases()`
    - this returns `TRUE`/`FALSE` for if the row has na values or not
```{r}
pain_sub1 <- na.omit(pain)
pain_sub2 <- pain[complete.cases(pain), ]
dim(pain_sub1)
dim(pain_sub2)
```

#### ----- Dates in R -----
**Symbol | Description**
%Y | Four-digit year
%y | Two-digit year
%m | Numeric month
%b% | Abbreviated name of month
%B | Full name of month
%d | Numeric day of the month
%H | Military time hour (24 hours)
%I | Imperial time hour (12 hours)
%M | Minute
%S | Seconds
%p | AM/PM

- to change a column to a date column, you would use the function `as.Date()` 
  - for columns with dates and times, you would use the function `as.POSIXct()`
```{r}
date_example <- data.frame(
  x = c(
    "2020-01-15", "2021-11-16",
    "2019-08-01"
  ),
  y = c(
    "2020-01-15 3:14 PM",
    "2021-11-16 5:00 AM",
    "2019-08-01 3:00 PM"
  ),
  z = c("04:10:00", "11:35:11", "18:00:45")
)
```
  - you also need a `format` and `tz` argument for both
```{r}
# convert date and date times using formats
date_example$x <- as.Date(date_example$x,
  format = "%Y-%m-%d",
  tz = "EST"
)
date_example$y <- as.POSIXct(date_example$y,
  format = "%Y-%m-%d %I:%M %p"
)

# add date to z and convert
date_example$z <- paste("2024-06-24", date_example$z)
date_example$z <- as.POSIXct(date_example$z,
  format = "%Y-%m-%d %H:%M:%S"
)
date_example
```
- once a column is made into a date column, you can use the function `difftime()` to find the time difference
  - you need `time1` and `time2` to find the difference (as `time1 - time2`) and then include the given `units`
```{r}
difftime(date_example$x[2], date_example$x[1], units = "days")
```

- you can also use the `seq()` function to add or subtract time by a specific unit:
```{r}
seq(date_example$x[1], by = "month", length = 3)
```
- another helpful package for manipulating time is the `lubridate` package

#### ----- Using Logic to Subset, Summarize, and Transform -----
the below are more logic operators that can be used in R
- `<` less than
- `<=` less than or equal to
- `>` greater than
- `>=` greater than or equal to
- `==` equal to
- `!=` not equal to
- `a %in% b` a’s value is in a vector of values b
```{r}
2 < 2
2 <= 2
3 > 2
3 >= 2
"A" == "B"
"A" != "B"
```
- there is a natural order between comparisons
```{r}
TRUE < FALSE # aka this will ask is 1 < 0
```

- the `%in%` operator checks whether a value is in a set of possible values
```{r}
1 %in% c(4, 1, 2)
c(0, 1, 5) %in% c(4, 1, 2)
```
- Additionally, we can use the following operators, which allow us to negate or combine logical operators.
  - `!x` - the NOT operator ! reverses TRUE/FALSE values
  - `x | y` - the OR operator | checks whether either x or y is equal to TRUE
  - `x & y` - the AND operator & checks whether both x and y are equal to TRUE
  - `xor(x,y)` - the xor function checks whether exactly one of x or y is equal to TRUE (called exclusive or)
  - `any(x)` - the any function checks whether any value in x is TRUE (equivalent to using an OR operator | between all values)
  - `all(x)` - the all function checks whether all values in x are TRUE (equivalent to using an AND operator & between all values)
```{r}
!(2 < 3)
("Alice" < "Bob") | ("Alice" < "Aaron")
("Alice" < "Bob") & ("Alice" < "Aaron")
xor(TRUE, FALSE)
any(c(FALSE, TRUE, TRUE))
all(c(FALSE, TRUE, TRUE))
```
```{r}
# the below example is checking those who do or do not have Medicaid and assigns a new value in that column
pain$MEDICAID_BIN[pain$MEDICAID_BIN == "no"] <- "No Medicaid"
pain$MEDICAID_BIN[pain$MEDICAID_BIN == "yes"] <- "Medicaid"
table(pain$MEDICAID_BIN)
# now if we want to include only those who have a follow-up, we would combine functions
pain_follow_up <- pain[!is.na(pain$PAIN_INTENSITY_AVERAGE.FOLLOW_UP), ] # this is asking for the data of the rows that don't have missing(na) values for the PAIN_INTENSITY_AVERAGE.FOLLOW_UP column
# we can now also use the any() function in combination with our previous column we created for lower back pain to see if there are any patients with general back pain (it will see if any of the columns comes back TRUE but we can use all() if we need all of the columns to come back TRUE)
pain$BACK <- any(
  pain$X208 == 1, pain$X209 == 1, pain$X212 == 1,
  pain$X213 == 1, pain$X218 == 1, pain$X219 == 1
)

# PRACTICE QUESTION
pain_subset <- pain[pain$PAIN_INTENSITY_AVERAGE >= 5, ]
head(pain_subset)

# if we want to now look at the patient race columns...
table(pain$PAT_RACE)
```
- the function `unique()` returns all the unique values in a column (which is especially useful for categorical columns)
```{r}
# most patients are either black or white
# you can also use the unique() function to see how many unique variables are in a column
unique(pain$PAT_RACE)
# we can combine some of the levels in this column using the %in% operator
aapi_values <- c(
  "CHINESE", "HAWAIIAN", "INDIAN (ASIAN)", "FILIPINO",
  "VIETNAMESE", "JAPANESE", "KOREAN", "GUAM/CHAMORRO",
  "OTHER ASIAN", "OTHER PACIFIC ISLANDER"
)
pain$PAT_RACE[pain$PAT_RACE %in% aapi_values] <- "AAPI"
pain$PAT_RACE[pain$PAT_RACE %in%
  c("ALASKA NATIVE", "AMERICAN INDIAN")] <- "AI/AN"
table(pain$PAT_RACE)
```
- the function `which()` is used to return the index values for all `TRUE` values
- the function `subset()` takes two arguments (the data (aka the vector, matrix, or dataframe), and a vector of TRUE/FALSE values to use for row selection)
```{r}
# we can use this with the race 'DECLINED' as not specified
pain$PAT_RACE[which(pain$PAT_RACE == "DECLINED")] <- "NOT SPECIFIED"
subset(pain, pain$PAT_RACE == "OTHER")
pain$PAT_RACE[pain$PATIENT_NUM == 3588] <- "NOT SPECIFIED"
table(pain$PAT_RACE)
```

## ----- Intro to Exploratory Data Analysis -----
- more packages to help with summary plots and tables are:
```{r}
install.packages("GGally")
install.packages("gt")
install.packages("gtsummary")

library(HDSinRdata)
library(GGally)
library(gt)
library(gtsummary)
```
```{r}
?NHANESsample
```
### ----- Univariate Distributions -----
```{r}
# first we want to see if there are any missing values
sum(complete.cases(NHANESsample)) # see if there are any complete rows
apply(NHANESsample, 2, function(x) sum(is.na(x))) / nrow(NHANESsample) # see which columns have the most missing values
nhanes_df <- na.omit(subset(NHANESsample,
  select = -c(SBP2, SBP3, SBP4, DBP2, DBP3, DBP4)
)) # here we are asking to create a subset of the df where it will only keep the columns besides the ones selected that have full data

# we can now look at some of the columns more in depth
table(nhanes_df$SMOKE)
summary(nhanes_df$YEAR)

# if we want to look at the most recent observations from the data, we would again use the subset() function
nhanes_df <- subset(nhanes_df, nhanes_df$YEAR == 2017)
```
- the function `ifelse()` is a logic operator that we can use to create a new vector with conditions
- to create a histogram (numeric values) and bar graphs (categorical values) and customize the plots
```{r}
# if we want to look at the most recent observations from the data, we would again use the subset() function
nhanes_df <- subset(nhanes_df, nhanes_df$YEAR == 2017)

# now we want to create a new column for if a person has ever smoked (combining the QuitSmoke and StillSmoke variables)
nhanes_df$EVER_SMOKE <- ifelse(nhanes_df$SMOKE %in% c(
  "QuitSmoke",
  "StillSmoke"
),
"Yes", "No"
)
table(nhanes_df$EVER_SMOKE)
# if we wanted to just see the output and not store it in a column, we could use the pipe operator `|>`
ifelse(nhanes_df$SMOKE %in% c("QuitSmoke", "StillSmoke"),
  "Yes", "No"
) |>
  table()

# now we want to look at the lead column
hist(log(nhanes_df$LEAD))
```
```{r}
# we can add to this visual by customizing the plot
hist(log(nhanes_df$LEAD),
  breaks = 30, col = "blue",
  main = "Histogram of Log Blood Lead Level",
  xlab = "Log Blood Lead Level"
)
```
```{r}
# for categorical columns we can use count of each variable
smoke_counts <- table(nhanes_df$SMOKE)
barplot(
  height = smoke_counts, names = names(smoke_counts),
  col = "violetred", xlab = "Smoking Status", ylab = "Frequency"
)

# you can even use multiple colors for each bar
barplot(
  height = smoke_counts, names = names(smoke_counts),
  col = c("orange", "violetred", "blue"),
  xlab = "Smoking Status", ylab = "Frequency"
)
```
```{r}
# PRACTICE QUESTION
lead_quantile_counts <- table(nhanes_df$LEAD_QUANTILE) / sum(table(nhanes_df$LEAD_QUANTILE))
barplot(
  height = lead_quantile_counts, names = names(lead_quantile_counts),
  col = "red", xlab = "Lead Quantile", ylab = "Percentage"
)
```
### ----- Bivariate Distributions -----
```{r}
# now if we want to look at the distribution of the categorical columns of smoking status by sex
table(nhanes_df$SMOKE, nhanes_df$SEX)
# and if we want to look at the numeric breakdown of lead by sex
summary(nhanes_df$LEAD[nhanes_df$SEX == "Female"])
summary(nhanes_df$LEAD[nhanes_df$SEX == "Male"])
```
```{r}
# we can also visually view a categorical column and continuous column by using boxplots with the plot() function
# the first argument for the function is the x-axis and the second is the y-axis
plot(nhanes_df$SEX, log(nhanes_df$LEAD),
  ylab = "Log Blood Lead Level",
  xlab = "Sex"
)
```
```{r}
# we can also combine categorical values in a more advanced boxplot
boxplot(log(LEAD) ~ SEX + EVER_SMOKE,
  data = nhanes_df,
  col = c("orange", "blue", "orange", "blue"),
  xlab = "Sex: Ever Smoked", ylab = "Log BLood Lead Level"
)
```
```{r}
# to see the distribution of two continuous columns, its best to use scatterplots
plot(nhanes_df$SBP1, nhanes_df$DBP1,
  col = "blue",
  xlab = "Systolic Blood Pressure",
  ylab = "Diastolic Blood Pressure"
)
```
```{r}
# since these two look highly correlated from the scatterplot, we can calculate the Pearson and Spearman correlation using the cor() function
# the default is Pearson but you can specify which correlation to use
cor(nhanes_df$SBP1, nhanes_df$DBP1)
cor(nhanes_df$SBP1, nhanes_df$DBP1, method = "spearman")
```
```{r}
# we can further customize the scatterplots by using another variable as the color gradient instead of specifying a color
plot(nhanes_df$SBP1, nhanes_df$DBP1,
  col = as.factor(nhanes_df$HYP),
  xlab = "Systolic Blood Pressure",
  ylab = "Diastolic Blood Pressure"
)
abline(v = 120, col = "blue") # these lines are to show the cutoff for the hypertension variable
abline(h = 80, col = "blue") # aka 120/80 is the cutoff for blood pressure
```
- to combine two plots in the same output, you would use the `par()` function that takes the arguments: `mfrow = c(nrow, ncol)`
  - this is asking to specify the number of columns and rows we want to use for the figure
```{r}
# we can also combine plots to display next to one another
par(mfrow = c(1, 2))
# boxplot
boxplot(log(LEAD) ~ HYP,
  data = nhanes_df, xlab = "Hypertension",
  ylab = "Log Blood Lead Level"
)
# Scatterplot
plot(nhanes_df$SBP1, nhanes_df$DBP1,
  col = as.factor(nhanes_df$HYP),
  xlab = "Systolic Blood Pressure",
  ylab = "Diastolic Blood Pressure"
)
abline(v = 120, col = "blue")
abline(h = 80, col = "blue")

# we do have to reset the par() for the rest of the displays
par(mfrow = c(1, 1))
```

```{r}
# PRACTICE QUESTION
table(nhanes_df$EDUCATION)
# need to plot 3 boxplots next to each other
par(mfrow = c(1, 3))
# boxplot 1
less_than_hs <- subset(nhanes_df, nhanes_df$EDUCATION == "LessThanHS")
boxplot(INCOME ~ BMI_CAT,
  data = less_than_hs,
  main = "Less than High School",
  xlab = "BMI Category",
  ylab = "Income"
)
# boxplot 2
hs <- subset(nhanes_df, nhanes_df$EDUCATION == "HS")
boxplot(INCOME ~ BMI_CAT,
  data = hs,
  main = "High School",
  xlab = "BMI Category",
  ylab = "Income"
)
# boxplot 3
more_than_hs <- subset(nhanes_df, nhanes_df$EDUCATION == "MoreThanHS")
boxplot(INCOME ~ BMI_CAT,
  data = more_than_hs,
  main = "More than High School",
  xlab = "BMI Category",
  ylab = "Income"
)


# then reset the par() again
par(mfrow = c(1, 1))
```

### ----- Autogenerated Plots ----- 
- the GGally package has useful functions for looking at multiple univariate and bivariate relationships at the same time
  - such as the `ggpairs()` function
```{r}
ggpairs(nhanes_df, columns = c("SEX", "AGE", "LEAD", "SBP1", "DBP1"))
```
    - this takes the data as the first argument and by default it plots the pairwise distributions for all columns but can be specified for only specific columns 
      - this is done using the `columns=` argument
- another useful package is the `ggcorr()` function which takes the arguments `label=TRUE`
  - this is essentially a correlation matrix for numeric values only so you can specify or just run on the entire df and it will only display the num correlations
```{r}
nhanes_df[, c("AGE", "LEAD", "SBP1", "DBP1")] |>
  ggcorr(label = TRUE)
```

### ----- Tables ----- 
- another way to view information is through a table
- the **gt** package and **gtsummary** package are used for that
```{r}
gt(head(nhanes_df[, c("ID", "AGE", "SEX", "RACE")]))
```
- the **tbl_summary()** function is from the gtsummary package and is used to summarize all the columns in the data 
  - it uses the argument `include` to specify which columns to include
  - the output can be piped into the `as_gt()` function
```{r}
tbl_summary(nhanes_df,
  include = c(
    "SEX", "RACE", "AGE", "EDUCATION", "SMOKE",
    "BMI_CAT", "LEAD", "SBP1", "DBP1", "HYP"
  )
) |>
  as_gt()
```
- if we want to change the type of data reported in the summary table, we would do so with the `statistic` argument
```{r}
tbl_summary(nhanes_df,
  include = c(
    "SEX", "RACE", "AGE", "EDUCATION", "SMOKE",
    "BMI_CAT", "LEAD", "SBP1", "DBP1", "HYP"
  ),
  by = "HYP",
  statistic = list(all_continuous() ~ "{mean} ({sd})")
) |>
  as_gt()
```

## -----  Data Transformations and Summaries ----- 
```{r}
# here will be using the dplyr package which is used for more extensive data exploration and transformation
library(HDSinRdata)
library(tidyverse)
data(NHANESsample)

class(NHANESsample)
```
- a **tibble** has all the properties of data frames but its a more modern version of a data frame
- the function `as_tibble()`
```{r}
nhanes_df <- as_tibble(NHANESsample)
print(head(nhanes_df))
nhanes_df <- as.data.frame(nhanes_df)
print(head(nhanes_df))
```
### ----- Subsetting Data -----
- we can only see the head of certain columns that we select
```{r}
select(nhanes_df, c(RACE, LEAD)) %>% head()
# we can remove columns with -c()
nhanes_df <- nhanes_df %>% select(-c(ID, LEAD_QUANTILE))
names(nhanes_df)
```
- if you want to filter the data to only observations after 2008
```{r}
nhanes_df_recent <- nhanes_df %>% filter(YEAR >= 2008)
```
```{r}
# Example 1: multiple filter calls
nhanes_df_males1 <- nhanes_df %>%
  filter(YEAR <= 2012) %>%
  filter(YEAR >= 2008) %>%
  filter(SEX == "Male")

# Example 2: combine with & operator
nhanes_df_males2 <- nhanes_df %>%
  filter((YEAR <= 2012) & (YEAR >= 2008) & (SEX == "Male"))

# Example 3: combine into one filter call with commas
nhanes_df_males3 <- nhanes_df %>%
  filter(between(YEAR, 2008, 2012), SEX == "Male")
```
- the `slice()` function to select a slice of rows by their index
```{r}
slice(nhanes_df, c(1, nrow(nhanes_df)))
```
- `slice_sample()` is a function that takes in an arguemnt `n` which specifies the number of random rows to sample from the data
- `slice_max()` and `slice_min()` are functions to specify  a column through the argument `order_by` and return the `n` rows with either the highest or lowest values in that column
```{r}
# three male observations with highest blood lead level in 2007
nhanes_df %>%
  filter(YEAR == 2007, SEX == "Male") %>%
  select(c(RACE, EDUCATION, SMOKE, LEAD, SBP1, DBP1)) %>%
  slice_max(order_by = LEAD, n = 3)

# three male observations with lowest blood lead level in 2007
nhanes_df %>%
  filter(YEAR == 2007, SEX == "Male") %>%
  select(c(RACE, EDUCATION, SMOKE, LEAD, SBP1, DBP1)) %>%
  slice_min(order_by = LEAD, n = 3)
```
### ----- Updating Rows and Columns -----
- the next functions are to update the rows and columns of the data itself
- the function `rename()` is to change the names of columns
```{r}
nhanes_df <- nhanes_df %>% rename(PIR = INCOME, SMOKE_STATUS = SMOKE)
names(nhanes_df())

ifelse(nhanes_df$SMOKE_STATUS == "NeverSmoke", "No", "Yes") %>%
  table()
```
- the function `case_when()` is an extension of `ifelse()` but allows to specify more than two cases 
```{r}
case_when(
  nhanes_df$SMOKE_STATUS == "NeverSmoke" ~ "Never Smoked",
  nhanes_df$SMOKE_STATUS == "QuitSmoke" ~ "Quit Smoking",
  nhanes_df$SMOKE_STATUS == "StillSmoke" ~ "Current Smoker"
) %>%
  table()
```
- the function `mutate()` takes a data frame and a set of columns with associated names to add to the data or update
```{r}
# the below we both create the new column and update the current column at the same time
nhanes_df <- nhanes_df %>%
  mutate(
    EVER_SMOKE = ifelse(SMOKE_STATUS == "NeverSmoke",
      "No", "Yes"
    ),
    SMOKE_STATUS =
      case_when(
        SMOKE_STATUS == "NeverSmoke" ~ "Never Smoked",
        SMOKE_STATUS == "QuitSmoke" ~ "Quit Smoking",
        SMOKE_STATUS == "StillSmoke" ~ "Current Smoker"
      )
  )

# the function `arrange()` takes in a data frame and a vector of columns used to sort the data
nhanes_df %>%
  select(c(YEAR, SEX, SMOKE_STATUS, SBP1, DBP1, LEAD)) %>%
  filter(SEX == "Male", SMOKE_STATUS == "Current Smoker") %>%
  arrange(desc(SBP1), desc(DBP1)) %>%
  head(8)
# the above is sorted by SBP1 and then by DBP1
# the below is sorting just by SBP1
nhanes_df %>%
  select(c(YEAR, SEX, SMOKE_STATUS, SBP1, DBP1, LEAD)) %>%
  filter(SEX == "Male", SMOKE_STATUS == "Current Smoker") %>%
  arrange(desc(SBP1)) %>%
  head(8)
```
```{r}
# PRACTICE QUESTION
nhanes_df %>%
  mutate(DBP_CHANGE = DBP4 - DBP1) %>%
  select(c(DBP1, DBP4, DBP_CHANGE)) %>%
  arrange(DBP_CHANGE) %>%
  head(4)
```
### ----- Summarizing and Grouping -----
- the `count()` function takes a data frame and one or more columns and counts the number of rows for each combination of unique values in these columns
```{r}
count(nhanes_df)
count(nhanes_df, RACE, YEAR)
```
- if we want to find the total number of observations as well as the mean and median systolic blood pressure for Non-Hispanic Blacks
```{r}
nhanes_df %>%
  filter(RACE == "Non-Hispanic Black") %>%
  summarize(
    TOT = n(), MEAN_SBP = mean(SBP1, na.rm = TRUE),
    MEAN_DBP = mean(DBP1, na.rm = TRUE)
  )
```
- the `group_by()` function takes a data frame and one or more columns with which to group the data
```{r}
nhanes_df %>%
  group_by(RACE) %>%
  slice(1)
# we can then combine functions and find the total number of observations as well as the mean systolic and diastolic blood pressure valuers for each racial groups
nhanes_df %>%
  group_by(RACE) %>%
  summarize(
    TOT = n(), MEAN_SBP = mean(SBP1, na.rm = TRUE),
    MEAN_DBP = mean(DBP1, na.rm = TRUE)
  )

# we can also then ungroup the data using the ungroup() function which restores the data to a single data frame
nhanes_df %>%
  select(SEX, RACE, SBP1, DBP1) %>%
  group_by(RACE) %>%
  ungroup() %>%
  arrange(desc(SBP1)) %>%
  slice(1)
```
### ----- CASE STUDY: Cleaning Tuberculosis Screening Data -----
```{r}
# the data we will be using is the tb_diagnosis_raw in the HDSinRdata package
# this data represents 1634 patients in rural South Africa who tested for TB
library(HDSinRdata)
library(tidyverse)
library(gt)
library(gtsummary)

# Read in data
data("tb_diagnosis_raw")

# Inspect variable descriptions
?tb_diagnosis_raw
```
```{r}
# the first thing we want to do is drop columns related to the participation in the survery and about seeking care
# there are also variables containing long or vague names so we will rename most of the variables
# Select variables and rename
tb_df <- tb_diagnosis_raw %>%
  select(c(
    xpert_status_fac, age_group, sex, hiv_status_fac,
    other_conditions_fac___1, other_conditions_fac___3,
    other_conditions_fac___88, other_conditions_fac___99,
    symp_fac___1, symp_fac___2, symp_fac___3, symp_fac___4,
    symp_fac___99, length_symp_unit_fac, length_symp_days_fac,
    length_symp_wk_fac, length_symp_mnt_fac, length_symp_yr_fac,
    smk_fac, dx_tb_past_fac, educ_fac
  )) %>%
  rename(
    tb = xpert_status_fac, hiv_pos = hiv_status_fac,
    cough = symp_fac___1, fever = symp_fac___2,
    weight_loss = symp_fac___3, night_sweats = symp_fac___4,
    symptoms_missing = symp_fac___99,
    ever_smoke = smk_fac,
    past_tb = dx_tb_past_fac, education = educ_fac
  )

tbl_summary(tb_df) %>%
  as_gt()
```
```{r}
# sometimes we also want to standard the variables used in each column
# for this example, there are some columns using 0/1 and others using 1/2 for boolean values
# re-code binary variables to 0/1 instead of 1/2
tb_df$tb <- case_when(
  tb_df$tb == 1 ~ "TB Positive",
  tb_df$tb == 2 ~ "TB Negative"
)
tb_df$male <- case_when(
  tb_df$sex == 1 ~ 1,
  tb_df$sex == 2 ~ 0
)
tb_df <- tb_df %>% select(-c(sex))

# Re-code diabetes to check if missing
tb_df$diabetes <- case_when(
  tb_df$other_conditions_fac___3 == 1 ~ 1,
  tb_df$other_conditions_fac___1 == 1 ~ 0,
  tb_df$other_conditions_fac___88 == 1 ~ NA,
  tb_df$other_conditions_fac___99 == 1 ~ NA,
  TRUE ~ 0
)
tb_df <- tb_df %>% select(-c(
  other_conditions_fac___1,
  other_conditions_fac___3,
  other_conditions_fac___88,
  other_conditions_fac___99
))
table(tb_df$diabetes)
```
```{r}
# Re-code variables with missing or refused values
tb_df$hiv_pos <- case_when(
  (tb_df$hiv_pos == 1) ~ 1,
  tb_df$hiv_pos %in% c(2, 77) ~ 0,
  tb_df$hiv_pos == 88 ~ NA
)

tb_df$ever_smoke <- case_when(
  tb_df$ever_smoke %in% c(1, 2) ~ 1,
  tb_df$ever_smoke == 3 ~ 0,
  tb_df$ever_smoke == 99 ~ NA
)

tb_df$past_tb <- case_when(
  tb_df$past_tb == 1 ~ 1,
  tb_df$past_tb %in% c(2, 77) ~ 0
)

# Code NA values and look at education distribution
tb_df$education[tb_df$education == 99] <- NA
hist(tb_df$education,
  xlab = "Years of Education",
  main = "Histograph of Education Years"
)
```
```{r}
# Categorize education to HS and above
tb_df$hs_less <- case_when(
  tb_df$education <= 12 ~ 1,
  tb_df$education > 12 ~ 0,
  TRUE ~ NA
)
# now we don't need the education column anymore so we will remove it
tb_df <- tb_df %>% select(-c(education))

# we now want to see how long someone has entered symptoms so we can look at the different units of time there are
tb_df %>%
  group_by(length_symp_unit_fac) %>%
  summarize(
    missing_days = sum(is.na(length_symp_days_fac)) / n(),
    missing_wks = sum(is.na(length_symp_wk_fac)) / n(),
    missing_mnt = sum(is.na(length_symp_mnt_fac)) / n(),
    missing_yr = sum(is.na(length_symp_yr_fac)) / n()
  )
```
```{r}
# we can also check to see if all the integers are positive or not
min(tb_df$length_symp_days_fac, na.rm = TRUE)
is.integer(tb_df$length_symp_days_fac)
```
```{r}
# we now want to create a new variable for whether or not a person has experienced symptoms for more than two weeks
# Categorize number of weeks experiencing symptoms
tb_df <- tb_df %>%
  mutate(two_weeks = case_when(
    (length_symp_unit_fac == 77 |
      is.na(length_symp_unit_fac)) ~ NA,
    (length_symp_unit_fac == 1 &
      length_symp_days_fac <= 14) ~ 0,
    (length_symp_unit_fac == 2 &
      length_symp_wk_fac <= 2) ~ 0,
    TRUE ~ 1
  ))
tb_df <- tb_df %>%
  select(-c(
    length_symp_wk_fac, length_symp_days_fac,
    length_symp_mnt_fac, length_symp_yr_fac,
    length_symp_unit_fac
  ))
```
```{r}
# we next want to create a new summary column that represents the total number of classic TB symptoms rather than keeping track of individual symptoms
# Count total number of symptoms
tb_df$num_symptoms <- tb_df$fever + tb_df$weight_loss + tb_df$cough +
  tb_df$night_sweats
tb_df$num_symptoms[tb_df$symptoms_missing == 1] <- NA
tb_df <- tb_df %>% select(-c(
  night_sweats, weight_loss, cough, fever,
  symptoms_missing
))

# Exclude observations with no TB symptoms
tb_df <- tb_df %>%
  filter(num_symptoms != 0)

table(tb_df$num_symptoms)
```
```{r}
# Convert all variables to factors
tb_df[] <- lapply(tb_df, function(x) {
  return(as.factor(x))
})

# we are now going to create a final summary table that includes all the summary statistics
tbl_summary(tb_df,
  by = "tb",
  label = list(
    tb = "Tuberculosis",
    age_group = "Age Group",
    hiv_pos = "HIV Positive",
    ever_smoke = "Ever Smoked",
    past_tb = "Past TB",
    male = "Male",
    hs_less = "High School or Less Educ.",
    two_weeks = "Two Weeks Symptoms",
    diabetes = "Diabetes",
    num_symptoms = "Number of Symptoms"
  )
) %>%
  add_overall() %>%
  as_gt() %>%
  cols_width(everything() ~ "55pt")
```
## ----- Merging and Reshaping Data -----
```{r}
library(tidyverse)
library(HDSinRdata)

data(covidcases) # data set about weekly COVID-19 case and death counts by county in the US for 2020
data(lockdowndates) # data set contains the start and end dates for statewide stay-at-home orders
data(mobility) # data set contains daily mobility estimates by state in 2020

head(covidcases)
head(mobility)
```
- the function `as.Date()` tells R to treat these columns as dates instead of characters
```{r}
mobility$date <- as.Date(mobility$date, formula = "%Y-%M-%D")
lockdowndates$Lockdown_Start <- as.Date(lockdowndates$Lockdown_Start,
  formula = "%Y-%M-%D"
)
lockdowndates$Lockdown_End <- as.Date(lockdowndates$Lockdown_End,
  formula = "%Y-%M-%D"
)
class(mobility$date)
class(lockdowndates$Lockdown_Start)
class(lockdowndates$Lockdown_End)

month(mobility$date[1])
week(mobility$date[1])
```
```{r}
# this is a test to show what the as.Date() and adding time calculations looks like
as.Date("2019-12-29") + weeks(1)
```
```{r}
covidcases$date <- as.Date("2019-12-29") + weeks(covidcases$week - 1) # this is now adding one week to this specific date
head(covidcases)
```

### ----- Tidy Data -----
- data is considered a **tidy** if:
  - each variable is associated with a single column
  - each observation is associated with a single row
  - each value has its own cell

```{r}
mat_mort1 <- data.frame(
  country = c(
    "Turkey", "United States",
    "Sweden", "Japan"
  ),
  y2002 = c(64, 9.9, 4.17, 7.8),
  y2007 = c(21.9, 12.7, 1.86, 3.6),
  y2012 = c(15.2, 16, 5.4, 4.8)
)
head(mat_mort1)
```
- the above isn't a tidy because the variable for maternity mortality rate is associated with multiple columns
- to make it a tidy, we can create separate columns for country, year, and maternity mortality rate
```{r}
mat_mort2 <- data.frame(
  country = rep(c("Turkey", "United States", "Sweden", "Japan"), 3),
  year = c(rep(2002, 4), rep(2007, 4), rep(2012, 4)),
  mat_mort_rate = c(64.0, 9.9, 4.17, 7.8, 21.9, 12.7, 1.86, 3.6, 15.2, 16, 5.4, 4.8))
head(mat_mort2)
```

### ----- Reshaping Data
- another function is `pivot_longer()` to change the data from what is called **wide form** to what is called **long form**
```{r}
# an example of this is if we want to take the lockdown data:
lockdown_long <- lockdowndates %>%
  pivot_longer(cols = c("Lockdown_Start", "Lockdown_End"),
               names_to = "Lockdown_Event", values_to = "Date") %>%
  mutate(Date = as.Date(Date, formula="%Y-%M-%D"),
         Lockdown_Event = ifelse(Lockdown_Event == "Lockdown_Start",
                                 "Start", "End")) %>%
  na.omit()
head(lockdown_long)
```
```{r}
# we can then take this data and transform it back to wider by using the `pivot_wider()` function
lockdown_wide <- pivot_wider(lockdown_long,
                             names_from = Lockdown_Event,
                             values_from = Date)
head(lockdown_wide)
```
```{r}
# now suppose that we want to create a data frame where the columns correspond to the number of cases for each state in New England
ne_cases <- covidcases %>%
  filter(state %in% c("Maine", "Vermont", "New Hampshire",
                      "Connecticut", "Rhode Island",
                      "Massachusetts")) %>%
  mutate(month = month(date)) %>%
  group_by(state, month) %>%
  summarize(total_cases = sum(weekly_cases)) %>%
  ungroup()
head(ne_cases)
```
- the above will give a warning message stating that the data is still grouped by state
```{r}
# we now want to convert this data to wider format with a column for each state
pivot_wider(ne_cases, names_from = state, values_from = total_cases)
```

### ----- Merging Data with Joins -----
- merging two data frames is called joining
```{r}
table1 <- data.frame(age = c(14, 26, 32),
                     name = c("Alice", "Bob", "Alice"))
table2 <- data.frame(name = c("Carol", "Bob"),
                     statins = c(TRUE, FALSE))
full_join(table1, table2, by = "name")
```
**Types of Joins:**
- left_join(table1, table2, by): Joins each row of table1 with all matches in table2.
- right_join(table1, table2, by): Joins each row of table2 with all matches in table1 (the opposite of a left join)
- inner_join(table1, table2, by): Looks for all matches between rows in table1 and table2. Rows that do not find a match are dropped.
- full_join(table1, table2, by): Keeps all rows from both tables and joins those that match. Rows that do not find a match have NA values filled in.
- semi_join(table1, table2, by): Keeps all rows in table1 that have a match in table2 but does not join to any information from table2.
- anti_join(table1, table2, by): Keeps all rows in table1 that do not have a match in table2 but does not join to any information from table2. The opposite of a semi-join.

```{r}
covidcases_full <- left_join(covidcases, lockdowndates,
                             by = c("state" = "State")) # sometimes we need to specify the joining column if its slightly worded differently
head(covidcases_full)
```

```{r}
# the `between()` function creates a new column
covidcases_full <- covidcases_full %>%
  mutate(lockdown = between(date, Lockdown_Start, Lockdown_End)) %>% # the new column 'lockdown' we created based on the two date columns
  select(-c(Lockdown_Start, Lockdown_End)) # now we don't need them, we drop these columns

covidcases_full %>%
  filter(state == "Alabama", county == "Jefferson",
         date <= as.Date("2020-05-10"))

covidcases_full <- inner_join(covidcases_full, mobility,
                              by = c("state", "date")) %>%
  select(-c(samples, m50_index))
head(covidcases_full)
```

```{r}
# PRACTICE QUESTION
df_A <- data.frame(patient_id = c(12, 9, 12, 8, 14, 8), 
                   visit_num = c(1, 1, 2, 1, 1, 2), 
                   temp = c(97.5, 96, 98, 99, 102, 98.6), 
                   systolic_bp = c(120, 138, 113, 182, 132, 146))
df_A
df_B <- data.frame(patient_id = c(12, 12, 12, 8, 8, 8, 14, 14), 
                   visit_num = c(1, 2, 3, 1, 2, 3, 1, 2),
                   digit_span = c(3, 5, 7, 7, 9, 5, 8, 7))
df_B

df_AB <- inner_join(df_A, df_B,
                    by = c("patient_id", "visit_num"))
head(df_AB)
```

## ----- Visualization with ggplot2 -----
```{r}
library(tidyverse)
library(HDSinRdata)
library(patchwork)
data(pain)
```

```{r}
# sampling data
set.seed(5)
pain_df_sub <- subset(pain,
                      select = -c(PAIN_INTENSITY_AVERAGE.FOLLOW_UP))

pain_df <- pain[complete.cases(pain_df_sub), ]
pain_df <- pain_df[sample(1:nrow(pain_df), 5000, replace = FALSE),]
```

### ----- Intro to ggplot -----
- the function `ggplot()` is to create a graph but using just the ggplot() will bring up an empty gray box that you build up with the code you put in it
```{r}
ggplot()
```
- the function `aes()` is to create aesthetics for the graph that you are making
```{r}
# geom_point() is for scatterplots
ggplot(pain_df) + geom_point(aes(x=PROMIS_ANXIETY,
                                 y=PROMIS_DEPRESSION))
# Alternative 1:
ggplot(pain_df, aes(x = PROMIS_ANXIETY, y = PROMIS_DEPRESSION)) +
  geom_point()
# Alternative 2:
ggplot() +
  geom_point(data = pain_df, aes(x = PROMIS_ANXIETY,
                                 y = PROMIS_DEPRESSION))
```
```{r}
# now the `labs()` function adds layers to the graph
ggplot(pain_df) +
  geom_point(aes(x = PROMIS_ANXIETY, y = PROMIS_DEPRESSION),
             color = "blue", size = 2, shape = 5) +
  labs(x = "PROMIS Anxiety Score", y = "PROMIS Depression Score",
       title = "Depression vs Anxiety Scores")
```

- other plotting options include `geom_histogram()` which has the options: `binwidth`, `y`, `alpha`, `color`, `fill`, `linetype`, `size`, and `weight`
  - by default: `y` in a histogram is the count for each bin
  - `theme_minimal()` function is used to change the background colors
  - `theme_bw()` function is the classic dark on light theme
```{r}
ggplot(pain_df) +
  geom_histogram(aes(x = PAIN_INTENSITY_AVERAGE), color = "violetred",
                 fill = "lightblue", alpha = 0.5, bins = 11) +
  labs(x = "Patient Reported Pain Intensity", y = "Count") +
  theme_minimal()
```
```{r}
# PRACTICE QUESTION
ggplot(pain_df, aes(x = GH_MENTAL_SCORE, y = PROMIS_SLEEP_DISTURB_V1_0)) +
  geom_smooth(color="navy", se=TRUE, fill = "lightblue")
```

### ----- Adjusting the Axes and Aesthetics -----

- the function `scale_x_continuous()` allows to specify the `limits`, `breaks`, and `labels`
```{r}
ggplot(pain_df) +
  geom_histogram(aes(x = PAIN_INTENSITY_AVERAGE), color = "violetred",
                 fill = "lightblue", alpha = 0.5, bins = 11) +
  labs(x = "Patient Reported Pain Intensity", y = "Count") +
  scale_x_continuous(breaks = 0:10) +
  theme_minimal()
```
- the `position="jitter"` added the to `geom_point()` function jitters the points making them not exactly line up to the lines making it easier to see trends
```{r}
ggplot(pain_df) +
  geom_point(aes(x = PROMIS_PHYSICAL_FUNCTION,
                 y = PROMIS_SLEEP_DISTURB_V1_0,
                 color = PAIN_INTENSITY_AVERAGE), position="jitter")
```
- the `scale_color_gradient()` function allows us to specify the low and high end colors
  - you can also combo this and use `scale_color_grandient2()` and `scale_color_grandientn()` to specify more color points
  - the function `coord_cartesian()` is used to specify the limits which clips the values rather than removing points outside the limits
```{r}
ggplot(pain_df) +
  geom_point(aes(x = PROMIS_PHYSICAL_FUNCTION,
                 y = PROMIS_SLEEP_DISTURB_V1_0,
                 color = PAIN_INTENSITY_AVERAGE),
             position = "jitter", alpha = 0.5) +
  scale_x_continuous(limits = c(15, 50), breaks = c(20, 30, 40, 50),
                     labels = c("-3 SD", "-2 SD", "-1 SD",
                                "Pop Mean")) +
  scale_y_continuous(breaks = c(40, 50, 60, 70, 80),
                     labels = c("-1 SD", "Pop Mean", "+1 SD", "+2 SD",
                                "+3 SD")) +
  scale_color_gradient(breaks = seq(0, 10, 2), low = "green",
                       high = "red", "Reported Pain") +
  labs(x = "PROMIS Physical Function T-Score",
       y = "PROMIS Sleep Distrubance T-Score") +
  theme_minimal()
```
```{r}
# now we want to create boxplots of the race categories
pain_df$PAT_RACE_CAT <- ifelse(pain_df$PAT_RACE %in% c("BLACK", "WHITE"),
                               pain_df$PAT_RACE, "OTHER")
pain_df$PAT_RACE_CAT <- as.factor(pain_df$PAT_RACE_CAT)

ggplot(pain_df) +
  geom_boxplot(aes(y = PAT_RACE_CAT, x = PAIN_INTENSITY_AVERAGE,
                   fill = PAT_RACE_CAT), alpha = 0.5) +
  theme_minimal()
```
```{r}
# the function `scale_y_discrete()` is a scale function that corresponds to a discrete y-asix
# we can also specify color using the color `palette` option
ggplot(pain_df)+
  geom_boxplot(aes(y = PAT_RACE_CAT, x = PAIN_INTENSITY_AVERAGE,
                   fill = PAT_RACE_CAT), alpha = 0.5) +
  scale_x_continuous(breaks = c(0:10)) +
  scale_y_discrete(limits = c("OTHER", "WHITE", "BLACK"),
                   labels = c("Other", "White", "Black")) +
  scale_fill_brewer(palette = "Dark2", guide = "none") +
  labs(x = "Reported Pain Intensity", y = "Reported Race") +
  theme_minimal()
```

- the RColorBrewer package contained other palette options
- you can use any of the palettes using `brewer.pal()`
```{r}
library(RColorBrewer)
display.brewer.all()
```

- the `scale_fill_manual()` function is used to specify the colors we want to use for the two categories using the `values` argument
```{r}
ggplot(pain_df) +
  geom_histogram(aes(x = PAIN_INTENSITY_AVERAGE, fill = "Baseline")) +
  geom_histogram(aes(x = PAIN_INTENSITY_AVERAGE.FOLLOW_UP,
                     fill = "Follow-Up")) +
  scale_x_continuous(breaks = c(0:10)) +
  scale_fill_manual(values = c("violetred", "pink"),
                    name = "Measurement") +
  labs(x = "Reported Pain Intensity", y = "Count") +
  theme_minimal()
```
### ----- Adding Groups -----
- there are other ways of adding multiple layers on one visual
```{r}
pain_df$HAS_FOLLOW_UP <- 
  !is.na(pain_df$PAIN_INTENSITY_AVERAGE.FOLLOW_UP)
ggplot(pain_df) +
  geom_density(aes(x = PROMIS_PHYSICAL_FUNCTION,
                   group = HAS_FOLLOW_UP,
                   color = HAS_FOLLOW_UP)) +
  scale_x_continuous(breaks = c(0:10)) +
  scale_color_discrete(name = "Follow-Up", labels = c("No", "Yes")) +
  labs(x = "PROMIS Physical Function T-Score",
       y = "Estimated Density") +
  theme_minimal()
```
```{r}
# to do another distribution, sometimes its helpful to look at the data to find the proportions
pain_df_grp <- pain_df %>%
  group_by(HAS_FOLLOW_UP, PAIN_INTENSITY_AVERAGE) %>%
  summarize(tot = n()) %>%
  mutate(prop = tot/sum(tot)) %>%
  ungroup()
head(pain_df_grp)
```
- the function `geom_col()` creates barplots
  ` to not have stack bars be default, you would use the argument `position=dodge` and thatll put the bars side-by-side
```{r}
ggplot(pain_df_grp) +
  geom_col(aes(x = PAIN_INTENSITY_AVERAGE, y = prop,
               fill = HAS_FOLLOW_UP)) +
  scale_x_continuous(breaks=c(0:10)) +
  scale_fill_discrete(name = "Seen at Follow Up",
                      labels = c("No", "Yes")) +
  labs(x = "Reported Pain Intensity", y = "Proportion") +
  theme_minimal()
```
- another way to add visual is to add a facet wrap to your ggplot object
  - facets divide a plot into subplots based on one or more discrete variable values
  - grouping the rows/columns we arrange uses the function `facet_grid()`
  - or you can wrap the plots into a rectangular format using `facet_wrap()`
```{r]}
ggplot(pain_df) +
  geom_histogram(aes(x = PAIN_INTENSITY_AVERAGE, fill = "Baseline")) +
  geom_histogram(aes(x = PAIN_INTENSITY_AVERAGE.FOLLOW_UP, 
                     fill = "Follow-Up")) +
  scale_x_continuous(breaks = c(0:10)) +
  scale_fill_manual(values = c("violetred", "pink"),
                    name = "Measurement") +
  labs(x = "Reported Pain Intensity", y = "Count") +
  facet_grid(row = vars(PAT_RACE_CAT)) +
  theme_minimal()
```

```{r}
# now we want to create another plot but first need to find the number of participants who selected each body region as well as the average pain intensity for those patients
# we also classify each body part region into larger groups 
pain_body_map <- data.frame(part = names(pain_df)[2:75])
pain_body_map$num_patients <- colSums(pain_df[, 2:75])
pain_body_map$perc_patients <- pain_body_map$num_patients / 
  nrow(pain_df)
pain_body_map$avg_pain <- colSums(pain_df[, 2:75] * 
                                    pain_df$PAIN_INTENSITY_AVERAGE) /
  pain_body_map$num_patients
pain_body_map <- pain_body_map %>% 
  mutate(region = case_when(
    part %in% c("X208", "X209", "X218", "X219", "X212",
                "X213") ~ "Back",
    part %in% c("X105", "X106", "X205", "X206") ~ "Neck",
    part %in% c("X107", "X110", "X207", "X210") ~ "Shoulders",
    part %in% c("X108", "X109", "X112", "X113") ~ "Chest/Abs",
    part %in% c("X126", "X127", "X228", "X229",
                "X131", "X132", "X233", "X234") ~ "Legs",
    part %in% c("X111", "X114", "X211", "X214", "X115", "X116",
                "X117", "X118", "X217", "X220") ~ "Arms",
    part %in% c("X119", "X124", "X221", "X226", "X125", "X128",
                "X227", "X230") ~ "Wrists/Hands",
    part %in% c("X215", "X216") ~ "Elbows",
    part %in% c("X135", "X136", "X237", "X238", "X133", "X134",
                "X235", "X236") ~ "Feet/Ankles",
    part %in% c("X129", "X130", "X231", "X232") ~ "Knees",
    part %in% c("X101", "X102", "X103", "X104", "X201", "X203",
                "X202", "X204") ~ "Head",
    part %in% c("X120", "X121", "X122", "X123", "X222", "X223",
                "X224", "X225") ~ "Hips"))

head(pain_body_map)
```
```{r}
# now we are plotting the average pain value for each body part as well as the proportion of patients who categorize it as painful
ggplot(pain_body_map) +
  geom_label(aes(x = perc_patients, y = avg_pain, label = part, 
                 color = region)) + 
  geom_hline(yintercept = mean(pain_body_map$avg_pain)) +
  annotate(geom = "text", label = "Average Pain Value", 
           x = 0.35, y = 7.0) + 
  labs(x = "Proportion Patients Selected Region", 
       y = "Average Pain of Patients") +
  theme_minimal()+
  theme(legend.position="bottom", 
        panel.grid.major = element_line(color = "lightpink"))
```

- after making all these plots, it is useful to save them
- the main function to do this is `ggsave()`
- the **patchwork** package is to incorporate multiple plots together
- the function `plot_layout()` function allows us to specify the grid used to arrange our figures
- another function `guide_area()` creates a placeholder for the legends
- and the argument `guide = "collect"` in the `plot_layout()` function specify that all guides should be put together
```{r}
p1 <- ggplot(pain_body_map) +
  geom_label(aes(x = perc_patients, y = avg_pain, label = part, 
                 color = region)) + 
  geom_hline(yintercept = mean(pain_body_map$avg_pain)) +
  annotate(geom = "text", label = "Average Pain Value", 
           x = 0.35, y = 7.0) + 
  labs(x = "Proportion of Patients Selecting Region", 
       y = "Average Pain of Patients") +
  scale_color_discrete(name="Body Part")+
  theme_minimal()+
  theme(legend.position = "bottom", 
        panel.grid.major = element_line(color = "lightpink"))

p2 <- ggplot(pain_body_map) +
  geom_histogram(aes(x = perc_patients), color = "violetred", 
                 fill = "lightpink") + 
  labs(x = "Proportion of Patients Selecting Region", y = "Count") +
  theme_minimal()+
  theme(panel.grid.major = element_line(color = "lightpink"))

p1 + p2 + guide_area() + plot_layout(ncol=1, guides = "collect",
                                     axes = "collect")

# ggsave("images/visualization_ggplot/myplot.png", height=10) 
```


## ----- CASE STUDY: Exploring Early COVID-19 Data -----
```{r}
install.packages('usmap')
library(HDSinRdata)
library(tidyverse)
library(patchwork)
library(gt)
library(gtsummary)
library(usmap)
```

### ----- Pre-processing -----
- we are going to be looking at COVID-19 cases and daily mobility statistics by state
```{r}
# Read in data
data(covidcases)
data(mobility)
```
- the first thing is to look at the columns in our data and correct date columns
```{r}
# Convert to date format and find week
mobility$date <- as.Date(mobility$date, formula = "%Y-%M-%D")
mobility$week <- week(mobility$date)
```
- now that we converted those to dates, we can summarize the mobility for a state across each week
```{r}
# Find average mobility for week
mobility_week <- mobility %>%
  group_by(state, week) %>%
  summarize(m50 = mean(m50, na.rm=TRUE), .groups = "drop") # the .groups argument is a grouping structure of the results where "drop" means all levels of grouping are dropped
head(mobility_week)
```
- for both datasets, we want to check whether each state was observed across all dates and how the state's name is represented
```{r}
# Find number of dates recorded for each state
table(mobility_week$state)
```
- for the `covidcases` data, the data is at the county level; we want to summarize this data instead
```{r}
# Find state names and number of weeks recorded for each state
unique(covidcases$state)

num_wks <- covidcases %>%
  group_by(state) %>%
  summarize(num_weeks = n_distinct(week), .groups = "drop")
summary(num_wks)
```
- we also need to update D.C. to be standard acrodd the data
```{r}
mobility_week$state[mobility_week$state == "Washington, D.C."] <- "District of Columbia"
```

- after checking the format of the state and week columns, we can now merge the data together using `left_join()`
```{r}
# Join cases and mobility data
covid <- left_join(covidcases, mobility_week, by = c("state", "week"))
```
- now we want to get some simple info about the continuous variables in the data 
- the main points we want to see if there are negative values or missing data
```{r}
summary(covid[, c("weekly_cases", "weekly_deaths", "m50")])
```
- there are negative numbers and are clear discrepancies in the data so we can either recode as 0 or NA depending on what we are trying to look for; for this instance, we will recode the negatives as NA
```{r}
# Set negative counts to NA
covid$weekly_cases <- replace(covid$weekly_cases,
                              which(covid$weekly_cases < 0),
                              NA)
covid$weekly_deaths <- replace(covid$weekly_deaths,
                               which(covid$weekly_deaths < 0),
                               NA)
```

- lastly, we want to add state abbreviations and region for each state using `state.name` and `state.region` vectors
```{r}
# Add region and abbreviation and remove county
region_key <- data.frame(state = c(state.name,
                                   "District of Columbia"),
                         state_abb = c(state.abb, "DC"),
                         region = c(as.character(state.region),
                                    "South"))

covid <- covid %>%
  left_join(region_key, by = c("state"))

head(covid)
```

### ----- Mobility and Cases Over Time -----
- now that the data is cleaned, we can start exploring the data
- we can create a summary table that'll show the measures differing by region
```{r}
covid %>%
  select(c("region", "m50", "weekly_cases", "weekly_deaths")) %>%
  tbl_summary(by = "region", missing = "no") %>%
  as_gt() %>%
  cols_width(everything() ~ "55pt")
```
- we can then plot this data
```{r}
# Average mobility in the US over time - overall 
pmob1 <- covid %>%
  select(c(region, state, week, m50)) %>%
  distinct() %>%
  group_by(week) %>%
  summarize(avg_m50 = mean(m50, na.rm=TRUE), .groups="drop") %>%
  ggplot() + 
  geom_line(aes(x = week, y = avg_m50)) +
  labs(x = "Week in 2020", y = "Average Mobility", 
       title = "Average Mobility in the US") + 
  theme_bw()

# Average mobility in the US over time - by region
pmob2 <- covid %>%
  select(c(region, state, week, m50)) %>%
  distinct() %>%
  group_by(region, week) %>%
  summarize(avg_m50 = mean(m50, na.rm=TRUE), .groups="drop") %>%
  ggplot() + 
  geom_line(aes(x = week, y = avg_m50, color = region)) +
  labs(x = "Week in 2020", y = "Average Mobility", 
       title = "Average Mobility by Region in the US",
       color = "Region") + 
  theme_bw() +
  theme(legend.position = "bottom") 

pmob1+pmob2
```

- so with these two graphs, we can see overall mobility and then by region
- the issue with this is we do not have population counts which would help standardize these numbers
- we can use a secondary y-axis using the `sec_axis()` function within the `scale_y_continuous()` allows us to plot deaths and cases together 
- for this case, the secondary axis is scaled by 1/10th of the primary axis
```{r}
# Change in number cases over time, per region
covid %>%
  filter(!is.na(region)) %>% 
  group_by(region, week) %>%
  summarize(weekly_cases = sum(weekly_cases, na.rm = TRUE),
            weekly_deaths = sum(weekly_deaths, na.rm = TRUE),
            .groups = "drop") %>%
ggplot() +
  geom_line(aes(x = week, y = weekly_cases, color = region, 
                linetype = "Cases")) + 
  geom_line(aes(x = week, y = weekly_deaths*10, color = region, 
                linetype = "Deaths")) +
  scale_y_continuous(name = "Average Weekly Cases",
                     sec.axis = sec_axis(~./10, 
                          name = "Average Weekly Deaths"))+
  scale_linetype(name = "Measure") +
  labs(x = "Week in 2020", color = "Region", 
       title = "Weekly Cases Over Time by Region") +
  theme_bw()
```

- now if we want to see how mobility and cases are related, we can look at scatter plots of mobility and cases in California
```{r}
covid_ca <- covid %>% filter(state == "California")
ggplot(covid_ca) +
  geom_point(aes(x = weekly_cases, y = m50), na.rm = TRUE) +
  labs(x = "Weekly Cases", y = "Average Mobility")
```

- there is a function `plot_usmap()` that is used to plot correlations by state
```{r}
# Calculate and plot correlation between cases and mobility, y state
covid_cor <- covid %>%
  group_by(state) %>%
  summarize(correlation = cor(weekly_cases, m50,
                              use = "complete.obs"))

plot_usmap(regions = "states", data = covid_cor, 
           values = "correlation") +
  scale_fill_gradient2(low = "darkblue", high = "darkred",
                       mid = "white", name = "Correlation") +
  labs(title = "Correlation Between Cases and Mobility") +
  theme(legend.position = "right")
```

- lastly, we will look at how the total cases and deaths are related to each other
  - this shows that the Northeast suffered more deaths per case overall
    - which may be related to the lower mobility and negative correlation between mobility and cases observed earlier

```{r}
# Relationship between cases and deaths summarized
covid %>%
  group_by(region, state_abb) %>%
  summarize(total_cases = sum(weekly_cases, na.rm = TRUE),
            total_deaths = sum(weekly_deaths, na.rm = TRUE),
            .groups = "drop") %>%
ggplot() + 
  geom_label(aes(x = total_cases, y = total_deaths, color = region,
                 label = state_abb), size = 1.5) + 
  labs(x = "Total Cases", y = "Total Deaths", color = "Region") +
  theme_bw()
```

## ----- Probability Distributions in R -----
- we are now going to practice probability distribution which is done by generating random samples and calculating the corresponding density, quantile, and cumulative functions that correspond to that distribution
```{r}
library(tidyverse)
library(HDSinRdata)
```

- now to draw random samples, we need to generate a code that changes the results everytime the code is run. 
- to replicade the code, we need to have a _random seed_ which makes the results the same everytime 
  - the `set.seed()` function takes a numeric seed value
```{r}
sample(1:10, 1) # the will change each time you run this chunk
set.seed(5)
sample(1:10, 1) # this will stay the same each time you run it
```

### ----- Probability Distribution Functions -----
- all of the common discrete (e.g., Bernoulli, binomial) and continuous (e.g. normal, uniform, exponential, Poisson) probability distributions have corresponding functions in R
- `r[dist]()`: random sample function for the given distribution (e.g., `rnorm()`, `runif()`)
- `d[dist]()`: density function for the distribution (e.g., `dnorm()`, `dunif()`)
- `p[dist]()`: cumulative distribution function for the distribution (e.g., `pnorm()`, `punif()`)
- `q[dist]()`: quantile function for the distribution (e.g., `qnorm()`, `qunif()`)

#### ----- Random Samples -----
- the below generates a sample of 100 random numbers following a normal distribution with mean 5 and standard deviation 1
  - the arguments for this function take in `n` (number of observations), `mean` (the mean with default value 0), and `sd` (the standard deviation with default value 1)

```{r}
x <- rnorm(n = 100, mean = 5, sd = 1)
hist(x)
```
- if we input a vector instead of a single value for the mean or sd if we want each sample to come from its own normal distribution

```{r}
x <- rnorm(n = 100, mean = rep(c(0,5), 50))
hist(x)
```
- for binomial distribution, the difference is that we need to specify a probability `p` and number of trials `size` (rather than `mean` and `sd` in the normal case)
- in the below, we generate 100 random numbers following a binomial distribution with 10 trials and a probability 0.5
```{r}
x <- rbinom(n = 100, p = 0.5, size = 10)
hist(x)
```
- can also specify a different size or probability of success for each sample
```{r}
x <- rbinom(n = 100, p = rep(c(0.25, 0.75), 50), size = 10)
hist(x)
```

#### ----- Density Function ------
- the next is density functions 
- the below code compares the values from some of the `dnorm()` function to the density equation and see if they are in fact equal
```{r}
dnorm(0) == 1/sqrt(2*pi)
dnorm(1) == exp(-1/2)/sqrt(2*pi)
dnorm(2) == exp(-1/2*2^2)/sqrt(2*pi)
```
- to get the density function for several values, we can create a vector
```{r}
dnorm(c(-1, 0, 1, 2, 3), mean = 1, sd = 2)
```

- for the binomial distribution, `dbinom()` returns the probability of a certain number of successes and correpsonds to the probability density function
```{r}
# we can find the probability of getting exactly 3 heads from 10 coin flips, each with a probability of 0.5 for heads
dbinom(3, size = 10, p = 0.5)
```

- `dnorm()` allows us to specify any continuous values for `x` but `dbinom()` gives a warning if x contains non-integer values since the support of a binomial variable only includes integers
```{r}
dbinom(2.4, size = 10, p = 0.5)
```
- we can also specify a vector for a distributions parameters to find the distribution function for different distributions
```{r}
dbinom(4, 4, size = 10, p = c(0.25, 0.5))
```

#### ----- Cumulative Distrbution -----
- for a normal distribution, the cumulative distribution is given by `pnorm()` which takes the arguements `x`, `mean`, and `sd` and returns the probability that a random variable following a _N(mean,sd)_ distribution is less than `x`.
```{r}
# verify the cumulative distribution with two different values of the mean
pnorm(0)
pnorm(5, mean = 5, sd = 1)
```

- binomial distributions are discrete and can only take on integer values from 0 to `size`
- this means that, for example, the `pbinom()` returns the same value for 3, 3.5, 3.6, up to (but not including) 4
```{r}
pbinom(c(3, 3.5, 3.6, 4), size = 10, p = 0.5)
```

- the parameters for the distrubtion can be varied by passing a vector for `size` and/or `p` to the cumulatize distribution function
```{r}
pbinom(c(3, 3, 4, 4), size = c(12, 10, 12, 10),
       p=c(0.25, 0.5, 0.25, 0.5))
```

#### ----- Quantile Distribution ------
- quantile distributions are the inverse of the cumulative distribution function
- this also takes `x`, `mean`, and `sd` and returns the value for which the cumulative distribution function is equal to `x`
  - this means that when `x` is equal to 0.5, the `qnorm()` function returns the median of the distribution, which is equal to the mean for the normal distribution
```{r}
qnorm(0.5)
qnorm(0.5, mean = 5, sd = 1)
```

- for the discrete binomial distribution, the `qbinom()` function returns the largest integer value for which the probability of being less than or equal to that value is at the most the inputted value x
```{r}
qbinom(c(0.2, 0.3), size = 10, p = 0.5)
```

#### ----- Reference List for Probability Distributions -----
- In the previous examples, we only used the normal and binomial distributions. The following list contains the other probability distributions available in R. For each distribution, we have given the arguments for the r[dist]() function. The other three functions have a similar format. Unless otherwise stated, the parameter n is the number of observations.

- **Beta**: rbeta(n, shape1, shape2, ncp = 0) with shape parameters shape1 and shape2 (and optional non-centrality parameter ncp).
- **Binomial**: rbinom(n, size, prob) with probability of success prob and number of trials size.
- **Cauchy**: rcauchy(n, location = 0, scale = 1) with location parameter location and scale parameter scale.
- **Chi-Square**: rchisq(n, df, ncp = 0) with df degrees of freedom and optional non-centrality parameter ncp.
- **Exponential**: rexp(n, rate = 1) with rate rate (i.e., mean = 1/rate).
F: rf(n, df1, df2, ncp) with df1 and df2 degrees of freedom (and optional non-centrality parameter ncp).
- **Gamma**: rgamma(n, shape, rate = 1, scale = 1/rate) with parameters shape and scale (or alternatively specified by rate).
- **Geometric**: rgeom(n, prob) with probability parameter prob.
- **Hypergeometric**: rhyper(nn, m, n, k) with m white balls, n black balls, and k balls chosen.
- **Logistic**: rlogis(n, location = 0, scale = 1) with parameters location and scale.
- **Log Normal**: rlnorm(n, meanlog = 0, sdlog = 1) with mean meanlog and standard deviation sdlog on the log scale.
- **Negative Binomial**: rnbinom(n, size, prob, mu) with parameters size and prob.
- **Normal**: rnorm(n, mean = 0, sd = 1) with mean equal to mean and standard deviation equal to sd.
- **Poisson**: rpois(n, lambda) with parameter lambda.
- **Student t**: rt(n, df, ncp) with df degrees of freedom (and optional non-centrality parameter ncp).
- **Uniform**: runif(n, min = 0, max = 1) with minimum value min and maximum value max.
- **Weibull**: rweibull(n, shape, scale = 1) with parameters shape and scale.
- **Wilcoxon Rank Sum**: rwilcox(nn, m, n) with nn number of observations and sample sizes m and n.
- **Wilcoxon Signed Rank**: rsignrank(nn, n) with nn number of observations and sample size n.

### ----- Empirical Distribution and Sampling Data -----
- the function used to sample from an empirical distribution: `sample(x, size, replace=FALSE, prob=NULL)`
  - this takes the values we want to sample from `x`, the number of observations we want to sample `size`, and whether we want to sample with replacement `replace`

```{r}
# we are going to sample 500 rows without replacement from the NHANESsample data 
nhanes_sample_ids <- sample(1:nrow(NHANESsample), 500, replace = FALSE)
nhanes_sample <- NHANESsample[nhanes_sample_ids, ]
dim(nhanes_sample) # get the dimensions of the new subset to make sure its 500
```

- we can now demonstrate the sampling with replacement
  - this is done by creating a new dataset that is sampled from the empirical distribution of the data called a _bootstrap sample_
```{r}
nhanes_sample_ids <- sample(1:nrow(NHANESsample), nrow(NHANESsample), 
                            replace = TRUE)
nhanes_sample <- NHANESsample[nhanes_sample_ids, ]
dim(nhanes_sample)
```

- another way to sapmle from a data frame is to use the `slice_sample()` function
  - using either the number of observations to sample is `n` or the proportion of observations to sample `prop`
  - can also sample with or without replacement by setting the value of the arguement `replace` (with default value FALSE)
```{r}
nhanes_sample <- NHANESsample %>%
  slice_sample(prop = 0.2, replace = FALSE)
dim(nhanes_sample)
```

- beyond sampling, we can also find the empirical cumulative distribution = use a given vector to infer a distribution
- to draw a random sample from a normal distribution (`vec`) and then find its empirical cumulative distribution using the `ecdf()` function
  - this function returns a function which can then be used to find the sample cumulative distribution for different values similar to the functions `p[dist]()`
```{r}
# lets see the sample probability that x <= 0
vec <- rnorm(100)
ecdf_vec <- ecdf(vec)
ecdf_vec(0)
```

- we can then plot the empirical distribution against the actual cdf using the `pnorm()` function
  - we create a sequence of possible `x` values to pass to both `pnorm()` and `ecdf_vec()`
```{r}
df <- data.frame(x = seq(-3, 3, 0.05))
df$ecdf <- ecdf_vec(df$x)
df$distn = pnorm(df$x)

ggplot(df) +
  geom_line(aes(x = x, y = ecdf), color = "black") +
  geom_line(aes(x = x, y = distn), color = "red")
```

## ----- Hypothesis Testing -----
- examples for hypothesis testing are testing for positive correlations, preforming two-sample paired t-tests, and testing for equal variance among groups

- we will be using the data from the Texas Health and Human Services Department and includes the reported number of induced terminations of pregnancy (ITOPs) from 2016 to 2021

```{r}
install.packages('car')

library(tidyverse)
library(car)
library(HDSinRdata)
library(gt)
library(gtsummary)

data(tex_itop)
```

#### ----- Univariate Distributions and One-Sample Tests -----

```{r}
# we first want to look at the number of induced terminations of pregnancy (ITOPs) in 2021 per 1000 females ages 15-49 in each country
county_rates_2021 <- tex_itop$total_rate[tex_itop$year == 2021]
hist(county_rates_2021, breaks = 35)
```
- from the above, we can see that there is a heavy-tailed distribution of the data and that there are very few total abortions but some have high abortion rates which indicates a small population
  - however, we can see that there are some countys that have both a high abortion count and high abortion rate

```{r}
tex_itop %>%
  filter(year == 2021) %>%
  slice_max(n = 10, total_rate) %>%
  dplyr::select(c(county, total_itop, total_rate))
```
- since some of these countys are so small, we can drop them from the analysis because their numbers could be considered outliers
  - for our one-sample analysis, however, we do not remove them 

```{r}
# lets look at the mean for out data
mean(county_rates_2021, na.rm = TRUE)
```
- we can also calculate a confidence interval for this mean too 
  - the confidence interval is a range of values used to estimate an unknown statistical parameter, in this case the mean
- for our data, we will calculate a 95% confidence interval
```{r}
est_mean <- mean(county_rates_2021, na.rm = TRUE) # the mean
est_sd <- sd(county_rates_2021) # the standard deviation
z_alpha <- dnorm(1 - 0.05 / 2) # this is the sample variance
n <- length(county_rates_2021) # this is the number of observations
c(est_mean - z_alpha * est_sd / sqrt(n), # this is the confidence interval formula
  est_mean + z_alpha * est_sd / sqrt(n))
```
- we can add the `round()` function to remove the trailing decimals of the number and the `paste()` function which creates a single character string from multiple inputs
```{r}
lower <- round(est_mean - z_alpha*est_sd/sqrt(n), 3)
upper <- round(est_mean + z_alpha*est_sd/sqrt(n), 3)
paste("Confidence Interval: (", lower, ",", upper, ")")
```
- now we want to run a hypothesis test to compare the mean to a pre-determined value
  - for this case it'll be the Texas Heartbeat Act introduced in 2021 which drastically reduced the number of eligible abortions
    - knowing that, we can test the number of abortions in 2021 compared to 2020 using a one-sided t-test
      - our null hypothesis is that mu >= 6.23 which is the mean abortion rate in 2020
    - we will use the `t.test()` function
      - the `x` is the sample
      - the `alternative` is the alternative hypothesis (default is a two-sided test)
      - the true value of the mean `mu` (default 0)
      - `conf.level` is the confidence level (default is 0.95)
```{r}
t.test(county_rates_2021, alternative = "less", mu = 6.23,
       conf.level=0.95)
```
- we can save these results
  - the object `t_test_res` is a list that contains information about the statistic, p-value, confidence interval, etc
```{r}
t_test_res <- t.test(county_rates_2021, alternative = "less",
                     mu = 6.23, conf.level = 0.95)
names(t_test_res)
```
```{r}
t_test_res$p.value
t_test_res$conf.int
```
- one thing to consider is that the `t.test()` function assumes that the sample `x` comes from a normal distribution
  - the one-sample Wilcoxon signed rank test is a non-parametric alternative to the one-sample t-test that can be used to compare the median value of a sample to a theoretical value without assuming that the data are normally distributed
    - this is done using the `wilcox.test()` function that takes the same arguments as the `t.test()` function
```{r}
# here we are testing that:
# Null Hypothesis (H₀): The true median of county_rates_2021 is equal to 5.14.
# Alternative Hypothesis (H₁): The true median of county_rates_2021 is less than 5.14 (this is specified by alternative = "less").
wilcox_res <- wilcox.test(county_rates_2021, alternative = "less",
                          mu = 5.14, conf.level = 0.95)
wilcox_res
# the output is going to tell us the:
# Test Statistic (V = 12807): A value calculated based on the ranks of the data. Higher values suggest the data is spread differently than the null hypothesis.
# p-value = 0.002: The probability of observing data as extreme (or more extreme) than what we have, if the null hypothesis were true.
wilcox_res$p.value
# the p-value is very small which means since it is less than 0.05, we reject the null hypothesis meaning that we have strong evidence that the true median of county_rates_2021 is less than 5.14
```

#### ----- Correlation and Covariance -----
- we now look at two-sample tests
```{r}
# looking at 2020 and 2021 rates by county
county_rates <- tex_itop %>%
  dplyr::select(c(county, total_rate, year)) %>% # first select the county, total rates and years
  filter(!(county %in% c("Terrell", "Loving")), # filter out Loving and Terrell counties to remove outliers
         year %in% c(2020, 2021)) %>% # only include years 2020 and 2021
  pivot_wider(names_from = year, values_from = total_rate) %>% # we want to now pivot the data wider to create the 2020 and 2021 rate columns
  na.omit() %>% # remove na values
  rename("y2020" = "2020", "y2021" = "2021") # rename the columns to the years
head(county_rates)
```

```{r}
ggplot(county_rates) +
  geom_point(aes(x = y2020, y = y2021)) +
  labs(x = "2020 ITOP Rates", y = "2021 ITOP Rates")

# from the below we can see there is a linear correlation between the two
```

- we can also calculate the covariance with the `cov()` 
```{r}
cor(county_rates$y2020, county_rates$y2021) # correlation
cov(county_rates$y2020, county_rates$y2021) # covariant
```
- the correlation being 0.5 means that the correlation is strong in the positive direction (out of -1 - 1)
  - the correlation is moderate showing that if a county had a high value in y2020 it tends to also have a high value in y2021
- the covariance being 5.2, since its positive means that the two variables move in the same direction but it doesn't tell the strength of the relationship without knowing the data
  - since we know the range is 0-~25, we can say its significant

- we can also test whether this correlation is significantly different from zero using the `cor.test()` 
```{r}
# we want to test whether there is a non-zero correlation between the 2020 and 2021 county rates using Pearson's product-moment correlation
cor_test_res <- cor.test(county_rates$y2020,
                         county_rates$y2021,
                         method = "pearson")
cor_test_res
# from these results, we can reject the null hypothesis that the correlation is zero and conclude that it is instead significantly different than zero
```
```{r}
cor_test_res$conf.int
```

#### ----- Two-Sample Tests for Continuous Variables -----
- to directly compare the difference between 2020 and 2021 using a two-sample test
```{r}
t_test_two_res <- t.test(x = county_rates$y2020,
                         y = county_rates$y2021)
t_test_two_res
# these results indicate that we reject the null hypothesis that the two county rates are equal to the 0.05 level

t_test_two_res$conf.int
```

- now if we want to compare the change in abortion rates from 2020 to 2021 between rural and urban counties
```{r}
# first we need to create a variable describing the rate change between these years
# we are going to use change in rate rather than percentage change to avoid infinite or undefined values
county_rates_type <- tex_itop %>%
  dplyr::select(c(county, urban, county_type, total_rate, year)) %>%
  filter(total_rate < 15, year %in% c(2020, 2021)) %>%
  pivot_wider(names_from = year, values_from = total_rate) %>%
  na.omit() %>%
  rename("y2020" = "2020", "y2021" = "2021") %>%
  mutate(rate_change = (y2021 - y2020))
```
- for this we are once again going to use a two-sample two-sided t-test, but this time the data are not paired
- below, we are going to use an alternative way to specify a t-test using a formula `lhs ~ rhs`   - `lhs` is a numeric column
  - `rhs` is a factor column with two levels
  - we also need to specify the data being used in this formula
```{r}
t_test_unpaired <- t.test(rate_change ~ urban, 
                          data = county_rates_type)
t_test_unpaired
# from these results we see we can reject the null hypothesis
# the rate changes for urban and rural counties are not significantly different

t_test_unpaired$estimate
# this is showing the estimated mean in both groups where we can confirm they aren't that different
```
```{r}
x <- county_rates_type$rate_change[county_rates_type$urban == 'Urban']
y <- county_rates_type$rate_change[county_rates_type$urban == 'Rural']
t.test(x = x, y = y, paired = FALSE) 
# these are the same results as above (just the other way of showing them)
```

- besides a t-test, we can run a two-sample Wilcoxon non-parametric test using the `wilcox.test()` function which can only compare two groups
  - if we want to compare two or more independent samples, we can use a Kruskal-Wallis rank sum test using the `kruskal.test()` functionor a one-way analysis of variance (ANOVA) using the `aov()` function

- for this test, we use `county_type` as an indicator for whether the county is urban, surburban, or rural
  - for the `kruskal.test()` we can either specify the arguemnts formula (`rate_change ~ county_type`) and data (`county_rates_type`)
    - or we can specify two vectors:
      - `x` as a numeric vector
      - `g` as a factor representing the group
```{r}
kruskal.test(county_rates_type$rate_change,
             county_rates_type$county_type)

aov_res <- aov(rate_change ~ county_type,
               data = county_rates_type)

# we need to run summary to get the p-value
summary(aov_res)

# both results are showing that we fail to reject the null hypothesis at the 0.05 level
```

#### ----- Two-Sample Variance Tests -----
- we can also test whether the variance of a continuous variable is equal between groups
  - we start to do this by comparing the variance in abortion rates in 2021 between urban and rural counties using an F-test
  - our null hypothesis for this test is that the variance in both groups is equal
  - the function `var.test()` implements the F-test
    - the arguements are the same for a `t.test()`; either using vectors `x` and `y` OR a `formula` and `data`
```{r}
f_test <- var.test(y2021 ~ urban, county_rates_type)
f_test
# we fail to reject the null hypothesis that the variances in rates are equal at the 0.05 level
# the estimate of the ratio of variance is around 1.11

f_test$estimate
```

- the last part of this is using the Levene's test to test whether group variances are equal when there are more than two groups
```{r}
levene_test <- leveneTest(y2021 ~ as.factor(county_type), 
                              county_rates_type)
print(levene_test)
# from these results, we reject the null hypothesis because the p-value is less than 0.05 (0.034)

levene_test[['Pr(>F)']] # access the p-value (here it has a different name)
```

#### ----- Two-Sample Tests for Categorical Variables -----
- now we are looking to compare the distributions of categorical variables
  - we will be comparing the ctounies by their abortion rates if it is above or below 11.2 (the national average that year)

```{r}
county_rates_type$below_nat_avg <- 
  ifelse(county_rates_type$y2020 > 11.2, "Above Nat Avg",
         "Below Nat Avg")
table(county_rates_type$below_nat_avg, county_rates_type$urban)
```
- now we can use a Fisher's exact test to test whether the classification of being above or below the national average and being rural and urban are associated with each other
  - for this, the null hypothesis is that the odds of being below the national average is equal between rural and urban counties
```{r}
fisher_test <- fisher.test(county_rates_type$urban,
                           county_rates_type$below_nat_avg)
fisher_test
# here we can cannot reject the null hypothesis and see that there is not a statistically significant difference between urban and rural counties at the 0.05 level (the odds ration is around 0.23)

fisher_test$estimate
```
- another test is a Pearson's Chi-Squared test, which can be used for large sample sizes
- since the counts for rural and urban counties are very small, we re-categorize our outcome to be at or above Texas' average to avoid this complication
- the `chisq.test()` function is to run this
  - it takes the arguments as a matrix or two factor vextors
  - another argument is `correct` (which the default is TRUE) which indicates whether to apply a continuity correction
```{r}
tex_mean <- mean(county_rates_type$y2020)
county_rates_type$below_tex_avg <- 
  ifelse(county_rates_type$y2020 > tex_mean, "Above Texas Ave",
         "Below Texas Ave")
table(county_rates_type$below_tex_avg, county_rates_type$urban)
```
```{r}
chi_sq <- chisq.test(county_rates_type$below_tex_avg,
                     county_rates_type$urban)
chi_sq
# from here we see a statistically significant difference in the proportion of counties above the national average between rural and urban counties and reject the null hypothesis at the 0.05 leve (our p-value is 0.009)
```

### ----- Adding Hypothesis Tests to Summary Tables -----
- when creating a stratified table, we can automatically add p-values for hypothesis tests comparing across populations using the `add_p()` function
  - by default, the function uses a Kruskal-Wallis rank sum test for continuous variables (or a Wilcoxon rank sum test when the `by` variable has two levels) and uses a Chi-Squared Contingency Table Test for categorical varaibles (or a Fisher's Exact Test for categorical variables with any expected cell count less than five)
    - the chosen tests are displayed as footnotes on the table
```{r}
tbl_summary(tex_itop, 
            by = "year",
            include = c(total_rate, white_rate, asian_rate, 
                        hispanic_rate, black_rate, native_american_rate),
            label = list(
              total_rate = "Overall",
              white_rate = "White",
              asian_rate = "Asian",
              hispanic_rate = "Hispanic",
              black_rate = "Black",
              native_american_rate = "Native American"),
            statistic = list(all_continuous() ~ "{mean} ({sd})")) %>%
  add_p() %>%
  modify_header(label = "**Variable**") %>%
  as_gt() %>%
  cols_width(label ~ "50pt",
             everything() ~ "30pt")
```

- from the table, we can see there wasn't any too statistically significant changes across years in the abortion rates for racial groups

## ----- CASE STUDY: Analyzing Blood Lead Level and Hypertension

```{r}
library(HDSinRdata)
library(tidyverse)
library(gt)
library(gtsummary)

data("NHANESsample")
```

- our analysis focuses on using hypothesis testing to look at the association between hypertension and blood lead levels by sex

- first we select demographic and clinical variables that we think will be significant
```{r}
NHANESsample <- NHANESsample %>%
  select("AGE", "SEX", "RACE", "SMOKE", "LEAD", "BMI_CAT",
         "HYP", "ALC") %>%
  na.omit()
```

- next, we want to look at a summary table by hypertension status
- we are expecting to see statistically significant differences between the two groups across all included variables
```{r}
tbl_summary(NHANESsample, by = c("HYP"),
            label = list(SMOKE ~ "SMOKING STATUS",
                         BMI_CAT ~ "BMI",
                         ALC ~ "ALCOHOL USE")) %>%
  add_p() %>%
  add_overall() %>%
  modify_spanning_header(c("stat_1", "stat_2") ~
                           "**Hypertension Status**") %>%
  as_gt() %>%
  cols_width(everything() ~ "55pt")
```

- we can also plot the distribution of blood lead leves (on a log scale) by sex and hypertension status
```{r}
ggplot(NHANESsample) +
  geom_boxplot(aes(x=LEAD,
                   y = interaction(HYP,SEX),
                   color = interaction(HYP,SEX))) +
  scale_x_continuous(trans = "log", breaks = c(0.1, 1, 10, 50)) +
  scale_y_discrete(labels = c("Male : 0", "Male : 1",
                              "Female : 0", "Female : 1")) +
  guides(color = "none") +
  labs(x="Blood Lead Level",
       y = "Sex : Hypertension Status")
```

- we hypothesize that log blood lead levels could be approximated by a normal distribution 
  - to test this hypothesis that there is a difference in mean log blood lead level between those with and without hypertension, we use a two-sample unpaired t-test
```{r}
t.test(log(LEAD) ~ HYP, data = NHANESsample)
# from these results, we can see that there is a statistical significants as the p-value is less than 0.05
```

- now we repeat this test for a stratified analysis and present the results in a concise table

```{r}
# stratify the data
nhanes_male <- NHANESsample[NHANESsample$SEX == "Male",]
nhanes_female <- NHANESsample[NHANESsample$SEX == "Female",]

# t-test for each
test_male <- t.test(log(LEAD) ~ HYP, data = nhanes_male)
test_female <- t.test(log(LEAD) ~ HYP, data = nhanes_female)

# create data frame
res_df <- data.frame(group = c("Male", "Female"),
                     statistic = signif(c(test_male$statistic,
                                          test_female$statistic), 3),
                     p.value = signif(c(test_male$p.value,
                                        test_female$p.value), 3))

res_df
```

## ----- Linear Regression -----
```{r}
library(HDSinRdata)
library(tidyverse)
library(broom)
library(car)

data(NHANESsample)
```

### ----- Simple Linear Regression -----
- with the sample data, we use linear regression to understand the association between blood lead levels and systolic blood pressure, adjusting for possible confounders
  - we first create summary columns for systolic and dystolic blood pressure 
    - if an observation has one blood pressure reading, than we use that value
      - if there is more than one blood pressure reading, then we drop the first observatin and average the rest
      - we do a complete case analysis by dropping any observation with NA values
```{r}
NHANESsample$SBP <- apply(NHANESsample[, c("SBP1", "SBP2", "SBP3",
                                          "SBP4")], 1,
                          function(x) case_when(sum(!is.na(x)) == 0 ~ NA,
                                                sum(!is.na(x)) == 1 ~ sum(x, na.rm = TRUE),
                                                sum(!is.na(x)) > 1 ~ mean(x[-1],
                                                                          na.rm = TRUE)))
NHANESsample$DBP <- apply(NHANESsample[,c("DBP1", "DBP2", "DBP3",
                                          "DBP4")], 1,
                          function(x) case_when(sum(!is.na(x)) == 0 ~ NA,
                                                sum(!is.na(x)) == 1 ~ sum(x, na.rm = TRUE),
                                                sum(!is.na(x)) > 1 ~ mean(x[-1],
                                                                          na.rm = TRUE)))
nhanes_df <- na.omit(subset(NHANESsample,
                            select = -c(SBP1, SBP2, SBP3, SBP4, DBP1,
                                        DBP2, DBP3, DBP4)))
dim(nhanes_df)
```

- now we make sure all the categorical variables are coded as factors
```{r}
nhanes_df$SEX <- as.factor(nhanes_df$SEX)
nhanes_df$RACE <- as.factor(nhanes_df$RACE)
nhanes_df$EDUCATION <- as.factor(nhanes_df$EDUCATION)
nhanes_df$BMI_CAT <- as.factor(nhanes_df$BMI_CAT)
nhanes_df$LEAD_QUANTILE <- as.factor(nhanes_df$LEAD_QUANTILE)
```

- to create our simple linear regression, we plot the relationship between blood lead level and systolic blood pressure
  - since we have a single continuous independent variable, a scatter plot allows us to easily visualize whether we meet the assumptions underlying linear regression
```{r}
plot(nhanes_df$LEAD, nhanes_df$SBP,
     xlab = "Blood Lead Level", ylab = "Systolic Blood Pressure",
     pch = 16)
```

- from the above plot, we don't see a linear relationship between the two variables

- even though there is no linear relationship, we can continue by fitting a simple linear regression model to explain the association
  - the function `lm(formula = y ~ x, data)` fits a linear model in R
```{r}
simp_model <- lm(formula = SBP ~ LEAD, data = nhanes_df)
summary(simp_model)
```
- we can visualize this estimated regression line and add it to the current scatter plot
```{r}
plot(nhanes_df$LEAD, nhanes_df$SBP,
     ylab = c("Systolic Blood Pressure"),
     xlab = c("Blood Lead Level"), pch = 16)
abline(simp_model, col = 2, lwd = 2) # this is where we can add our new line from above
```

### ----- Multiple Linear Regression -----
- we will create another model but add additional variables 
```{r}
adj_model <- lm(SBP ~ LEAD + AGE + SEX, data = nhanes_df)
summary(adj_model)
```

- we can also extract the estimated regression coefficients from the model using the `coef()` function or by using the `tidy()` function from the **broom** package
  - this function puts the coefficient estimates, standard errors, statistics, and p-values in a data frame
    - can also add a confidence interval by specifying `conf.int = TRUE` 
```{r}
coef(adj_model)
```

```{r}
tidy(adj_model, conf.int = TRUE, conf.level = 0.95)
```
- other useful summary functions are: 
  - `resid()` which returns the residual values for the model 
  - `fitted()` which returns the fitted values or estimated y values
  - `predict()` is what we can use to predict on new data
```{r}
summary(resid(adj_model))
```

```{r}
plot(nhanes_df$SBP, fitted(adj_model),
     xlab = "True Systolic Blood Pressure",
     ylab = "Predicted Systolic Blood Pressure", pch = 16)
abline(a = 0, b = 1, col = "red", lwd = 2)
```

- we can then perform a nested hypothesis test between our sample linear regression model and our adjusted model using the `anova()` function
  - we pass both models along with the arguemnt `test="F"` to indicate that we are performing an F-test.
```{r}
print(anova(simp_model, adj_model, test="F"))
```
- the model summary for the adjusted model displays the estimated coefficient for sex as 'SEXFemale' which indicates that the reference level for sex is male. 
  - we can change the reference level by reordering the factor variable either by using the `factor()` function and specifying 'Female' as the first level or by using the `relevel()` function
```{r}
nhanes_df$SEX <- relevel(nhanes_df$SEX, ref = "Female") # we are releveling the SEX column with Female as the new reference level
adj_model2 <- lm(SBP ~ LEAD + AGE + SEX, data = nhanes_df)
tidy(adj_model2)
```
- the `lm()` function also allows to use `.` to indicate that we would like to include all remaining columns as independent variables or the `-` to exclude variables
```{r}
lm(SBP ~ . - ID - RACE - EDUCATION - INCOME - SMOKE - YEAR - BMI_CAT - 
     LEAD_QUANTILE - DBP - ALC - HYP - RACE, data = nhanes_df)
```

### ----- Diagnostic Plots and Measures -----
- in the previous plot, our model doesn't have a great fit
- there are several build in plots to help further diagnostic plots
```{r}
par(mfrow = c(2, 2)) # plot all four plots together
plot(adj_model)

# the last plot summarizes the outliers, leverage, and influential points
```

#### ----- Normality -----

- the `qqnorm()` and `qqline()` functions can take in the residuals from our model as an argument
```{r}
par(mfrow = c(1, 2)) # plot next to each other
hist(resid(adj_model), xlab = "Residuals",
     main = "Histogram of Residuals")
qqnorm(resid(adj_model))
qqline(resid(adj_model), col = "red")

# both the histogram and qq-plot show the residuals are positively skewed, and thus the assumption of normality is not satisfied for our residuals
```

- instead of direct residuals, we can also find standardized residuals with the function `rstandard()`
  - the standard residuals are the raw residuals divided by an estimate of the standard deviation for the residual which is different for each observation
```{r}
par(mfrow = c(1, 2))
hist(rstandard(adj_model), xlab = "Standardized Residuals",
     main = "Histogram of Standardized Residuals",
     cex.main = 0.65)
qqnorm(rstandard(adj_model), cex.main = 0.65)
qqline(rstandard(adj_model), col = "red")
```

#### ----- Homoscedasticity, Linearity, and Collinearity -----
- we can also create a residual vs fitted plot or plot the residuals against included covariates
  - both plots, we are looking for the points to be spread roughly evenly around 0 with no discerning pattern
```{r}
par(mfrow = c(1, 2))
plot(fitted(adj_model), resid(adj_model),
     xlab = "Fitted Values", ylab = "Residuals")
plot(nhanes_df$LEAD, resid(adj_model),
     xlab = "Blood Lead Level", ylab = "Residuals")

# however, both of these plots show a funnel shape, indicating a growing and shrinking variance of residuals by level
# this indicates that we are violating the homoscedasticity assumption
```
- to quantify any collinearity between the included covariates, we can calculate the variance inflation factors 
- the `vif()` function allows us to calculate the variance inflation factors or generalized variance inflation factors for all covariates
```{r}
vif(adj_model)
# the output shows that all VIF values are around 1, indicating low levels of collinearity
```

#### ----- Leverage and Influence -----
- leverage values measure how much an individual observation's y value influences its own predicted value and indicate whether observations have extreme predictor values compared to the rest of the data 
  - leverage values range from 0 to 1 and sum to the number of estimated coefficients
  - observations with high leverage have the potential to significantly impact the estimated regression coefficients and the overall fit of the model
    - therefore, examining leverage values helps identify observations that may be influenced or outliers
```{r}
sort(hatvalues(adj_model), decreasing = TRUE)[1:10]
# this is going to show us the ten highest leverage values
```
```{r}
nhanes_df[order(hatvalues(adj_model), decreasing = TRUE),] %>%
  select(c(SBP, LEAD, AGE, SEX)) %>%
  head(10)
```

- some other measures of influence are the DFBETAs and Cook's distance, which measure how much each observation influences the estimated coefficients and the estimated y values
  - the `influence.measures()` functino provides a set of measures that quantify the influence of each observation on a linear regression model
    - the output returns the values in a matrix called `infmat`
```{r}
inf_mat <- influence.measures(adj_model)[['infmat']]
as.data.frame(inf_mat) %>% head()
```

### ----- Interactions and Transformations -----
- to improve the models, we can make transformations
  - if we first can consider log transformations
```{r}
par(mfrow=c(2,2))
hist(nhanes_df$SBP, xlab = "Systolic Blood Pressure",
     main = "")
hist(log(nhanes_df$SBP), xlab = "Log Systolic Blood Pressure",
     main = "")
hist(nhanes_df$LEAD, xlab = "Blood Lead Level",
     main = "")
hist(log(nhanes_df$LEAD), xlab = "Log Blood Lead level",
     main = "")
```

- now the above output graphs have a more symmetrical distribution

- to add a transformation to a model, you can simply apply `lm()` to the formula
```{r}
model_nlog_nlog <- lm(SBP ~ LEAD + AGE + SEX, data = nhanes_df)
model_log_nlog <- lm(log(SBP) ~ LEAD + AGE + SEX, data = nhanes_df)
model_nlog_log <- lm(SBP ~ log(LEAD) + AGE + SEX, data = nhanes_df)
model_log_log <- lm(log(SBP) ~ log(LEAD) + AGE + SEX, data = nhanes_df)

summary(model_nlog_nlog)$adj.r.squared
summary(model_log_nlog)$adj.r.squared
summary(model_nlog_log)$adj.r.squared
summary(model_log_log)$adj.r.squared
```
```{r}
par(mfrow=c(2,2))
qqnorm(rstandard(model_nlog_nlog), main = "Original Model") 
qqline(rstandard(model_nlog_nlog), col = "red")
qqnorm(rstandard(model_log_nlog), main = "Log SBP") 
qqline(rstandard(model_log_nlog), col = "red")
qqnorm(rstandard(model_nlog_log), main = "Log Lead") 
qqline(rstandard(model_nlog_log), col = "red")
qqnorm(rstandard(model_log_log), main = "Log SBP, Log Lead") 
qqline(rstandard(model_log_log), col = "red")
```
- both indicate that the model with the log-log transformation is the best fit, though the model with just a log transformation for `SBP` has a similar qq-plot

- another consideration we can try is polynomial transformations
  - the `poly(x, degree=1)` function allows us to specify a polynomial transformation where we might have higher degree terms
```{r}
model_poly <- lm(SBP ~ poly(LEAD, 3) + AGE + SEX, data = nhanes_df)
model_poly
```

- we can summarize the outcome for our log-log model using the `tidy()` function
```{r}
tidy(model_log_log)
# we can see small p-values for each estimated coefficient
```
- another component to add is an interaction term
  - for this data we can consider an interaction between sex and blood lead level
    - you can add an interaction tot he formula using a `:` between the two variables
```{r}
model_interaction <- lm(log(SBP) ~ log(LEAD) + AGE + SEX +
                          SEX:log(LEAD), data = nhanes_df)
summary(model_interaction)
# our output shows that the coefficient for this interaction is indeed significant
```

### ----- Evaluation Metrics -----
- there are a few other metrics that can help us understand how well our model fits the data and can also help with model selection
  - the `AIC()` function finds the Akaike information criterion (AIC)
    - takes into account both the goodness of fit (captured by the likelihood of the model) and the complexity of the model (captured by the number of parameters used)
      - lower AIC values are preferable
  - the `BIC()` function finds the Bayesian information criterion (BIC)
    - similar to AIC but has a stronger penalty for model complexity compared to AIC
  - both balance the trade-off between model complexity and goodness of fit
```{r}
AIC(model_log_log)
AIC(model_interaction)
```
```{r}
BIC(model_log_log)
BIC(model_interaction)
```


































