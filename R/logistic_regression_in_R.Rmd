---
title: "logistic_regression_in_R"
output: html_document
---

# Logistic Regression in R

[Source](https://www.youtube.com/watch?v=C4N3_XJJ-jU)

For this example, we are going to get a real dataset from the UCI machine learning repository:

<https://archive.ics.uci.edu/ml/index.php>

Specifically, we want the Heart Disease Dataset:

<https://archive.ics.uci.edu/ml/datasets/Heart+Disease>

## Load Libraries and Data

```{r}
library(ggplot2)
library(cowplot)
```

```{r}
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"

data <- read.csv(url, header=FALSE) # this is how we read the dataset into R from the URL

head(data)
```

Since there are no column names in the data, we name the columns after the names that were listed on the UCI website

```{r}
colnames(data) <- c(
  "age",
  "sex",# 0 = female, 1 = male
  "cp", # chest pain
  # 1 = typical angina,
  # 2 = atypical angina,
  # 3 = non-anginal pain,
  # 4 = asymptomatic
  "trestbps", # resting blood pressure (in mm Hg)
  "chol", # serum cholestoral in mg/dl
  "fbs",  # fasting blood sugar if less than 120 mg/dl, 1 = TRUE, 0 = FALSE
  "restecg", # resting electrocardiographic results
  # 1 = normal
  # 2 = having ST-T wave abnormality
  # 3 = showing probable or definite left ventricular hypertrophy
  "thalach", # maximum heart rate achieved
  "exang",   # exercise induced angina, 1 = yes, 0 = no
  "oldpeak", # ST depression induced by exercise relative to rest
  "slope", # the slope of the peak exercise ST segment
  # 1 = upsloping
  # 2 = flat
  # 3 = downsloping
  "ca", # number of major vessels (0-3) colored by fluoroscopy
  "thal", # this is short of thalium heart scan
  # 3 = normal (no cold spots)
  # 6 = fixed defect (cold spots during rest and exercise)
  # 7 = reversible defect (when cold spots only appear during exercise)
  "hd" # (the predicted attribute) - diagnosis of heart disease
  # 0 if less than or equal to 50% diameter narrowing
  # 1 if greater than 50% diameter narrowing
)
head(data)
```

However, the `str()` function, which describes the **str**ucture of the data, tells us that some of the columns are messed up

```{r}
str(data)
# ex: we see that sex is supposed to be a factor (not a number) where 0=female and 1=male
# another issue is that two of the columns have "?" values in them when they should be NA
```

### Clean the Data

```{r}
# First thing we want to tackle is changing the "?"s to NAs
data[data == "?"] <- NA 

# then we want to convert the "sex" column from 0/1 to F/M and factor it
data[data$sex == 0,]$sex <- "F"
data[data$sex == 1,]$sex <- "M"
data$sex <- as.factor(data$sex)

# then we convert other columns to factors too 
data$cp <- as.factor(data$cp)
data$fbs <- as.factor(data$fbs)
data$restecg <- as.factor(data$restecg)
data$exang <- as.factor(data$exang)
data$slope <- as.factor(data$slope)

# since the "ca" column originally had a "?" in it, it things its a column of strings so we have to correct it to a column of integers and then convert to a factor
data$ca <- as.integer(data$ca)
data$ca <- as.factor(data$ca)
# then do the same for "thal"
data$thal <- as.integer(data$thal)
data$thal <- as.factor(data$thal)

# the column "hd" needs to be converted to a factor and also from 0/1 to healthy/unhealthy so for this we do a trick of using the `ifelse()` function
data$hd <- ifelse(test=data$hd == 0, yes="Healthy", no="Unhealthy")
data$hd <- as.factor(data$hd)

# then check to see everything was changed correctly 
str(data)
```

### Check the data

Lets first see how many samples (rows of data) have NA values so we can see if we can toss these samples out or impute values for the NAs

```{r}
nrow(data[is.na(data$ca) | is.na(data$thal),])
```

To view the specific rows of data with the NA:

```{r}
data[is.na(data$ca) | is.na(data$thal),]
```

For this example, we will just remove these samples

```{r}
nrow(data) # we see there are 303 total rows with samples
# then remove the rows with NA:
data <- data[!(is.na(data$ca) | is.na(data$thal)),]

nrow(data) # we now have the total without the samples we just removed
```

Now, we need to make sure that healthy and diseased samples come from each gender (female and male)

```{r}
# using the `xtabs()` function, we pass the data and use the "model syntax" to select the columns in the data we want to build a table from 
xtabs(~ hd + sex, data=data)
```

Next, let's check to see there is a wide enough spread of chest pain aka "cp" reported by patients

```{r}
xtabs(~ hd + cp, data=data)
```

Then, we do the same thing for all the boolean and categorical variables that we are using to predict heart disease.

```{r}
xtabs(~ hd + fbs, data=data)
xtabs(~ hd + restecg, data=data) # this is showing potential issues of only 4 patients reporting `1` for the data 
xtabs(~ hd + slope, data=data)
xtabs(~ hd + ca, data=data)
xtabs(~ hd + thal, data=data)

```

## Simple Logistic Regression

We will be using the `glm()` function that performs **G**eneralized **L**inear **M**odels

For this case, we are specifying that we want to use sex to predict heart disease (**hd**).

We also want to specify that we want the **binomial** family of generalized linear models. This makes the **glm()** function do Logistic Regression, as apposed to some other type of generalized linear model.

```{r}
logistic <- glm(hd ~ sex, data=data, family="binomial")
summary(logistic) # this is how we get details about the logistic regression
```

Coefficients: Estimate Std. Error z value Pr(\>\|z\|)\
(Intercept) -1.0438 0.2326 -4.488 7.18e-06 ***sexM 1.2737 0.2725 4.674 2.95e-06*** \>the coefficient outputs correspond to the following model: heart disease = -1.0438 + 1.2737 x the patient is male(1) for female odds: heart disease = -1.0438 + 1.2737 x 0 which is reduced to heart disease = -1.0438 which means that the log(odds) that a female has heart disease = -1.0438

A small p-value alone isn't interesting, we also want large effect sizes, which is what the log(odds) and log(odds ratio) tell us

In logistic regression, we estimate the mean of the data, and the variance is derived from the mean. Since we are not estimating the variance from the data (and, instead, just deriving it from the mean), it is possible that the variance is underestimated. If so, we can adjust the dispersion parameter in the `summary()` command.

```         
Null deviance: 409.95  on 296  degrees of freedom
```

Residual deviance: 386.12 on 295 degrees of freedom \>These can be used to compare models, compute R\^2 and an overall p-value

AIC: 390.12 \>The AIC (Akaike Information Criterion), in this context is just the Residual Deviance adjusted for the number of parameters in the model usually used to compare one model to another

Number of Fisher Scoring iterations: 4 \>tells us how quickly the glm() function converged on the maximum likelihood estimates for the coefficients

## Fancy Logistic Regression

```{r}
# we can go from using one variable to predict heart disease to using all the variables to predict heart disease
logistic <- glm(hd ~ ., data=data, family="binomial") # using '.' indicates all
summary(logistic)
```

If we want to calculate **McFadden's Pseudo R\^2**, we can pull the **log-likelihood of the null model** out of the **logistic** variable by getting the value for the **null deviance** and dividing by -2

```{r}
ll.null <- logistic$null.deviance/-2
```

and we can pull the **log-likelihood for the fancy model** out of the **logistic** variable by getting the value for the **residual deviance** and dividing by -2

```{r}
ll.proposed <- logistic$deviance/-2
```

then after doing the math, we can get a pseudo R\^2 that can be interpreted as the overall effect size

```{r}
(ll.null - ll.proposed) / ll.null
```

and we can use those same log-likelihoods to calculate a p-value for that R\^2 using a Chi-square distribution

```{r}
1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic$coefficients)-1))
```

In this case, since the p-value is tiny, the R\^2 value isn't due to dumb luck

## Graph

We can also draw a graph that shows the predicted probabilities that each patient has heart disease along with their actual heart disease status

```{r}
# To start drawing the graph, we need to create a new data.frame that contains the probabilities of having heart disease along with the actual heart disease status
predicted.data <- data.frame(
  probability.of.hd=logistic$fitted.values,
  hd=data$hd)

# then we sort the data.frame from low probabilities to high probabilities
predicted.data <- predicted.data[
  order(predicted.data$probability.of.hd, decreasing=FALSE),]

# then we add a new column to the data.frame that has the rank of each sample from low to high probability
predicted.data$rank <- 1:nrow(predicted.data)

# then we need to load the libraries
library(ggplot2)
library(cowplot)

# then we call the plot using geom_point() to draw the data
ggplot(data=predicted.data, aes(x=rank, y=probability.of.hd)) +
  geom_point(aes(color=hd), alpha=1, shape=4, stroke=2) +
  xlab("Index") +
  ylab("Predicted probability of getting heart disease")

# and then we save the graph
ggsave("D:/GitHub/important-reference-repo/Images/heart_disease_probabilities.pdf")
```
