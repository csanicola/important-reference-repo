ggplot(smaller, aes(x = carat, y = price)) +
geom_bin2d()
library(hexbin)
install.packages("hexbin")
library(hexbin)
ggplot(smaller, aes(x = carat, y = price)) +
geom_hex()
ggplot(smaller, aes(x = carat, y = price)) +
geom_boxplot(aes(group = cut_width(carat, 0.1)))
# first create the dataframe which we will be working with
mouse.data <- data.frame(
weight=c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
size=c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3)
)
# you can also just load in an existing dataframe too
# first create the dataframe which we will be working with
mouse.data <- data.frame(
weight=c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
size=c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3)
)
# you can also just load in an existing dataframe too
mouse.data
# Now, we want to look at a basic plot of the data to see what it is doing
plot(mouse.data$weight, mouse.data$size)
# we can now set up the linear regression
mouse.regression <- lm(size ~ weight, data=mouse.data)
# we can now set up the linear regression
# we call the lm(linear model) function, and pass it a formula and the mouse data
# the formula is `y-values = y-intercept + slope x x-values`
# size = y-intercept + slope x weight
mouse.regression <- lm(size ~ weight, data=mouse.data)
summary(mouse.regression)
abline(mouse.regression, col="blue")
# first create the dataframe which we will be working with
mouse.data <- data.frame(
weight=c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
size=c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3)
)
# you can also just load in an existing dataframe too
mouse.data
# Now, we want to look at a basic plot of the data to see what it is doing
plot(mouse.data$weight, mouse.data$size)
# we can now set up the linear regression
# we call the lm(linear model) function, and pass it a formula and the mouse data
# the formula is `y-values = y-intercept + slope x x-values`
# size = y-intercept + slope x weight
mouse.regression <- lm(size ~ weight, data=mouse.data)
summary(mouse.regression)
# now, we can add the regression line to the graph from before
abline(mouse.regression, col="blue")
# now, we can add the regression line to the graph from before
plot(mouse.data$weight, mouse.data$size)
abline(mouse.regression, col="blue")
# first create the dataframe which we will be working with
mouse.data <- data.frame(
weight=c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
size=c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3),
tail=c(0.7, 1.3, 0.7, 2.0, 3.6, 3.0, 2.9, 3.9, 4.0)
)
# you can also just load in an existing dataframe too
mouse.data
plot(mouse.data$weight, mouse.data$size)
simple.regression <- lm(size ~ weight, data=mouse.data)
simple.regression <- lm(size ~ weight, data=mouse.data)
# in R, we specify the relationship we are looking for by using the `~` character
# in this example, we specify size is predicted by weight
summary(simple.regression)
plot(mouse.data$weight, mouse.data$size)
abline(simple.regression, col="red", lwd=2)
plot(mouse.data)
multiple.regression <- lm(size ~ weight + tail, data=mouse.data)
# the actual formula being used is: `size = y-intercept + slope1 x weight + slope2 x tail`
# we are specifying that size is predicted by weight and tail
summary(multiple.regression)
data.matrix <- matrix(nrow=100, ncol=10) # we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
data.matrix <- matrix(nrow=100, ncol=10) # we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
head(data.matrix)
# now that we have our fake data, our goal is to draw a plot that will show how our data is related to one another
# by default: `prcomp()` expects the samples to be rows and the genes to be columns
# since our matrix has the samples as columns and rows as genes (variables), we need to transpose the matrix using the `t()` function
# without transposing the data, we will get a graph that shows how the genes are related to one another (which is not our goal here)
pca <- prcomp(t(data.matrix), scale=TRUE)
plot(pca$x[,1], pca$x[,2]) # here we are using the first two columns in x to draw a 2-D plot that uses the first two PCs
pca.var <- pca$dev^2
# to see how much variation is in the original data each principal component (PC) accounts for, we use the square of standard deviation (`sdev`)
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
# the percentages of how much the variation accounts for is much more important than the actual number so we then calculate the percentages
pca.var <- pca$dev^2
# to see how much variation is in the original data each principal component (PC) accounts for, we use the square of standard deviation (`sdev`)
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
# the percentages of how much the variation accounts for is much more important than the actual number so we then calculate the percentages
# we can then plot these percentages with a barplot
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
data.matrix <- matrix(nrow=100, ncol=10) # we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
head(data.matrix)
# now that we have our fake data, our goal is to draw a plot that will show how our data is related to one another
# by default: `prcomp()` expects the samples to be rows and the genes to be columns
# since our matrix has the samples as columns and rows as genes (variables), we need to transpose the matrix using the `t()` function
# without transposing the data, we will get a graph that shows how the genes are related to one another (which is not our goal here)
pca <- prcomp(t(data.matrix), scale=TRUE)
plot(pca$x[,1], pca$x[,2]) # here we are using the first two columns in x to draw a 2-D plot that uses the first two PCs
# the first column is PC1 so that's on the x-axis and the second column is PC2 so that's on the y-axis
# once we plot the data, we can see there is a cluster of data on the right side and on the left side
# to get a sense of how meaningful these clusters are, we next want to see how much variation in the original data PC1 accounts for
pca.var <- pca$dev^2
# to see how much variation is in the original data each principal component (PC) accounts for, we use the square of standard deviation (`sdev`)
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
# the percentages of how much the variation accounts for is much more important than the actual number so we then calculate the percentages
# we can then plot these percentages with a barplot
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
pca.var <- pca$sdev^2
# to see how much variation is in the original data each principal component (PC) accounts for, we use the square of standard deviation (`sdev`)
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
# the percentages of how much the variation accounts for is much more important than the actual number so we then calculate the percentages
# we can then plot these percentages with a barplot
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
# we can also make a ggplot that is fancier and supplies much more information
library(ggplot2)
pca.data <- data.frame(Sample=rownames(pca$x), # one column with the sample ids
X=pca$x[,1], # these two columns for the X
Y=pca$x[,2]) # and Y coordinates for each sample
pca.data
# we can now call to the `ggplot()` function
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("My PCA Graph")
# we then want to look at the loading scores to see which genes have the largest effect on where samples are plotted in the PCA plot
# the `prcomp()` function calls the loading scores rotations
# each PC has loading scores but since PC1 accounts for 92% of the variation in the data, we will just be looking at the loading scores from PC1
# genes that push samples to the left side of the graph will have large negative values and genes that push the samples to the right side of the graph have large positive values
# since we want to look at both samples, we are going to use the `abs()` (absolute value) function to sort based on the number's magnitude rather than from high to low
gene_scores <- abs(loading_scores)
# we then want to look at the loading scores to see which genes have the largest effect on where samples are plotted in the PCA plot
# the `prcomp()` function calls the loading scores rotations
# each PC has loading scores but since PC1 accounts for 92% of the variation in the data, we will just be looking at the loading scores from PC1
loading_scores <- pca$rotation[,1]
# genes that push samples to the left side of the graph will have large negative values and genes that push the samples to the right side of the graph have large positive values
# since we want to look at both samples, we are going to use the `abs()` (absolute value) function to sort based on the number's magnitude rather than from high to low
gene_scores <- abs(loading_scores)
gene_score_ranked <- sort(gene_scores, decreasing=TRUE) # we now want to sort the magnitudes of the loading scores from high to low
# we then want to look at the loading scores to see which genes have the largest effect on where samples are plotted in the PCA plot
# the `prcomp()` function calls the loading scores rotations
# each PC has loading scores but since PC1 accounts for 92% of the variation in the data, we will just be looking at the loading scores from PC1
loading_scores <- pca$rotation[,1]
# genes that push samples to the left side of the graph will have large negative values and genes that push the samples to the right side of the graph have large positive values
# since we want to look at both samples, we are going to use the `abs()` (absolute value) function to sort based on the number's magnitude rather than from high to low
gene_scores <- abs(loading_scores)
gene_score_ranked <- sort(gene_scores, decreasing=TRUE) # we now want to sort the magnitudes of the loading scores from high to low
top_10_genes <- names(gene_score_ranked[1:10]) # we can now get the names of the top 10 genes with the largest loading score magnitudes
top_10_genes
# we can also then see which of these genes have negative loading scores (which push the "wt" samples to the left side of the graph) and which have positive loading scores (push the "wt" samples to the right side of the graph)
pca$rotation[top_10_genes,1] ## show the scores (and +/- sign)
# for a fancier looking plot, we will be using ggplot2
library(ggplot2)
data.matrix <- matrix(nrow=100, ncol=10) # we are creating the same fake data as we did in the PCA example;we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
head(data.matrix)
pca <- prcomp(t(data.matrix), scale=TRUE, center=TRUE)
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("PCA Graph")
# for a fancier looking plot, we will be using ggplot2
library(ggplot2)
data.matrix <- matrix(nrow=100, ncol=10) # we are creating the same fake data as we did in the PCA example;we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
head(data.matrix)
pca <- prcomp(t(data.matrix), scale=TRUE, center=TRUE)
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("PCA Graph")
distance.matrix <- dist(scale(t(data.matrix), center=TRUE, scale=TRUE), # just like with PCA, we need to transpose the matrix; we also center and scale the measurements for each gene which are now the columns in the data
method="euclidean") # we are also telling the `dist()` function that we want it to create the matrix using the Euclidean distance metric; this function has 6 different methods to choose from
mds.stuff <- cmdscale(distance.matrix, eig=TRUE, x.red=TRUE)
mds.stuff <- cmdscale(distance.matrix, eig=TRUE, x.ret=TRUE)
mds.var.per <- round(mds.stuff$eig/sum(mds.stuff$eig)*100, 1)
mds.data <- data.frame(Sample=rownames(mds.values),
X=mds.values[,1],
Y=mds.values[,2])
mds.values <- mds.stuff$points
mds.data <- data.frame(Sample=rownames(mds.values),
X=mds.values[,1],
Y=mds.values[,2])
ggplot(data=mds.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
theme_bw() +
xlab(paste("MDS1 - ", mds.var.per[1], "%", sep="")) +
ylab(paste("MDS2 - ", mds.var.per[2], "%", sep="")) +
ggtitle("MDS plot using Euclidean distance")
log2.data.matrix <- log2(data.matrix)
# first we create an empty matrix
log2.distance.matrix <- matrix(0,
nrow=ncol(log2.data.matrix),
ncol=ncol(log2.data.matrix),
dimnames=list(colnames(log2.data.matrix),
colnames(log2.data.matrix)))
# first we create an empty matrix
log2.distance.matrix <- matrix(0,
nrow=ncol(log2.data.matrix),
ncol=ncol(log2.data.matrix),
dimnames=list(colnames(log2.data.matrix),
colnames(log2.data.matrix)))
# then we fill the matrix with the average of the absolute values of the log fold changes
for(i in 1:ncol(log2.distance.matrix)) {
for(j in 1:i) {
log2.distance.matrix[i, j] <-
mean(abs(log2.data.matrix[,i] - log2.data.matrix[,j]))
}
}
# first we create an empty matrix
log2.distance.matrix <- matrix(0,
nrow=ncol(log2.data.matrix),
ncol=ncol(log2.data.matrix),
dimnames=list(colnames(log2.data.matrix),
colnames(log2.data.matrix)))
# then we fill the matrix with the average of the absolute values of the log fold changes
for(i in 1:ncol(log2.distance.matrix)) {
for(j in 1:i) {
log2.distance.matrix[i, j] <-
mean(abs(log2.data.matrix[,i] - log2.data.matrix[,j]))
}
}
log2.distance.matrix
# for a fancier looking plot, we will be using ggplot2
library(ggplot2)
data.matrix <- matrix(nrow=100, ncol=10) # we are creating the same fake data as we did in the PCA example;we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
head(data.matrix)
pca <- prcomp(t(data.matrix), scale=TRUE, center=TRUE)
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("PCA Graph")
distance.matrix <- dist(scale(t(data.matrix), center=TRUE, scale=TRUE), # just like with PCA, we need to transpose the matrix; we also center and scale the measurements for each gene which are now the columns in the data
method="euclidean") # we are also telling the `dist()` function that we want it to create the matrix using the Euclidean distance metric; this function has 6 different methods to choose from
mds.stuff <- cmdscale(distance.matrix, eig=TRUE, x.ret=TRUE)
mds.var.per <- round(mds.stuff$eig/sum(mds.stuff$eig)*100, 1)
mds.values <- mds.stuff$points
mds.data <- data.frame(Sample=rownames(mds.values),
X=mds.values[,1],
Y=mds.values[,2])
ggplot(data=mds.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
theme_bw() +
xlab(paste("MDS1 - ", mds.var.per[1], "%", sep="")) +
ylab(paste("MDS2 - ", mds.var.per[2], "%", sep="")) +
ggtitle("MDS plot using Euclidean distance")
log2.data.matrix <- log2(data.matrix)
# first we create an empty matrix
log2.distance.matrix <- matrix(0,
nrow=ncol(log2.data.matrix),
ncol=ncol(log2.data.matrix),
dimnames=list(colnames(log2.data.matrix),
colnames(log2.data.matrix)))
# then we fill the matrix with the average of the absolute values of the log fold changes
for(i in 1:ncol(log2.distance.matrix)) {
for(j in 1:i) {
log2.distance.matrix[i, j] <-
mean(abs(log2.data.matrix[,i] - log2.data.matrix[,j]))
}
}
log2.distance.matrix
# for a fancier looking plot, we will be using ggplot2
library(ggplot2)
data.matrix <- matrix(nrow=100, ncol=10) # we are creating the same fake data as we did in the PCA example;we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
head(data.matrix)
pca <- prcomp(t(data.matrix), scale=TRUE, center=TRUE)
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("PCA Graph")
distance.matrix <- dist(scale(t(data.matrix), center=TRUE, scale=TRUE), # just like with PCA, we need to transpose the matrix; we also center and scale the measurements for each gene which are now the columns in the data
method="euclidean") # we are also telling the `dist()` function that we want it to create the matrix using the Euclidean distance metric; this function has 6 different methods to choose from
mds.stuff <- cmdscale(distance.matrix, eig=TRUE, x.ret=TRUE)
mds.var.per <- round(mds.stuff$eig/sum(mds.stuff$eig)*100, 1)
mds.var.per
mds.values <- mds.stuff$points
mds.data <- data.frame(Sample=rownames(mds.values),
X=mds.values[,1],
Y=mds.values[,2])
mds.data
ggplot(data=mds.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
theme_bw() +
xlab(paste("MDS1 - ", mds.var.per[1], "%", sep="")) +
ylab(paste("MDS2 - ", mds.var.per[2], "%", sep="")) +
ggtitle("MDS plot using Euclidean distance")
log2.data.matrix <- log2(data.matrix)
# first we create an empty matrix
log2.distance.matrix <- matrix(0,
nrow=ncol(log2.data.matrix),
ncol=ncol(log2.data.matrix),
dimnames=list(colnames(log2.data.matrix),
colnames(log2.data.matrix)))
# then we fill the matrix with the average of the absolute values of the log fold changes
for(i in 1:ncol(log2.distance.matrix)) {
for(j in 1:i) {
log2.distance.matrix[i, j] <-
mean(abs(log2.data.matrix[,i] - log2.data.matrix[,j]))
}
}
log2.distance.matrix
mds.stuff <- cmdscale(as.dist(log2.distance.matrix),
eig=TRUE,
x.ret=TRUE)
mds.var.per <- round(mds.stuff$eig/sum(mds.stuff$eig)*100, 1)
mds.var.per
mds.values <- mds.stuff$points
mds.data <- data.frame(Sample=rownames(mds.values),
X=mds.values[,1],
Y=mds.values[,2])
mds.data
ggplot(data=mds.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
theme_bw() +
xlab(paste("MDS1 - ", mds.var.per[1], "%", sep="")) +
ylab(paste("MDS2 - ", mds.var.per[2], "%", sep="")) +
ggtitle("MDS plot using avg(logFC) as the distance")
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"
data <- read.csv(url, header=FALSE) # this is how we read the dataset into R from the URL
head(data)
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"
data <- read.csv(url, header=FALSE) # this is how we read the dataset into R from the URL
head(data)
colnames(data) <- c(
"age",
"sex",# 0 = female, 1 = male
"cp", # chest pain
# 1 = typical angina,
# 2 = atypical angina,
# 3 = non-anginal pain,
# 4 = asymptomatic
"trestbps", # resting blood pressure (in mm Hg)
"chol", # serum cholestoral in mg/dl
"fbs",  # fasting blood sugar if less than 120 mg/dl, 1 = TRUE, 0 = FALSE
"restecg", # resting electrocardiographic results
# 1 = normal
# 2 = having ST-T wave abnormality
# 3 = showing probable or definite left ventricular hypertrophy
"thalach", # maximum heart rate achieved
"exang",   # exercise induced angina, 1 = yes, 0 = no
"oldpeak", # ST depression induced by exercise relative to rest
"slope", # the slope of the peak exercise ST segment
# 1 = upsloping
# 2 = flat
# 3 = downsloping
"ca", # number of major vessels (0-3) colored by fluoroscopy
"thal", # this is short of thalium heart scan
# 3 = normal (no cold spots)
# 6 = fixed defect (cold spots during rest and exercise)
# 7 = reversible defect (when cold spots only appear during exercise)
"hd" # (the predicted attribute) - diagnosis of heart disease
# 0 if less than or equal to 50% diameter narrowing
# 1 if greater than 50% diameter narrowing
)
head(data)
str(data)
library(ggplot2)
library(cowplot)
# First thing we want to tackle is changing the "?"s to NAs
data[data == "?"] <- NA
# then we want to convert the "sex" column from 0/1 to F/M and factor it
data[data$sex == 0,]$sex <- "F"
data[data$sex == 1,]$sex <- "M"
data$sex <- as.factor(data$sex)
# then we convert other columns to factors too
data$cp <- as.factor(data$cp)
data$fbs <- as.factor(data$fbs)
data$restecg <- as.factor(data$restecg)
data$exang <- as.factor(data$exang)
data$slope <- as.factor(data$slope)
# since the "ca" column originally had a "?" in it, it things its a column of strings so we have to correct it to a column of integers and then convert to a factor
data$ca <- as.integer(data$ca)
data$ca <- as.factor(data$ca)
# then do the same for "thal"
data$thal <- as.integer(data$thal)
data$thal <- as.factor(data$thal)
# the column "hd" needs to be converted to a factor and also from 0/1 to healthy/unhealthy so for this we do a trick of using the `ifelse()` function
data$hd <- ifelse(test=data$hd == 0, yes="Healthy", no="Unhealthy")
data$hd <- as.factor(data$hd)
# then check to see everything was changed correctly
str(data)
nrow(data[is.na(data$ca) | is.na(data$thal),])
data[is.na(data$ca) | is.na(data$thal),]
nrow(data) # we see there are 303 total rows with samples
# then remove the rows with NA:
data <- data[!(is.na(data$ca) | is.nda(data$thal)),]
nrow(data) # we see there are 303 total rows with samples
# then remove the rows with NA:
data <- data[!(is.na(data$ca) | is.na(data$thal)),]
nrow(data) # we now have the total without the samples we just removed
# using the `xtabs()` function, we pass the data and use the "model syntax" to select the columns in the data we want to build a table from
xtabs(~ hd + sex, data=data)
xtabs(~ hd + cp, data=data)
xtabs(~ hd + fbs, data=data)
xtabs(~ hd + restecg, data=data) # this is showing potential issues of only 4 patients reporting `1` for the data
xtabs(~ hd + slope, data=data)
xtabs(~ hd + ca, data=data)
xtabs(~ hd + thal, data=data)
logistic <- glm(hd ~ sex, data=data, family="binomial")
logistic <- glm(hd ~ sex, data=data, family="binomial")
summary(logistic) # this is how we get details about the logistic regression
# we can go from using one variable to predict heart disease to using all the variables to predict heart disease
logistic <- glm(hd ~ ., data=data, family="binomial") # using '.' indicates all
summary(logistic)
ll.null <- logistic$null.deviance/-2
ll.proposed <- logistic$deviance/-2
(ll.null - ll.proposed) / ll.null
1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic$coefficients)-1))
# To start drawing the graph, we need to create a new data.frame that contains the probabilities of having heart disease along with the actual heart disease status
predicted.data <- data.frame(
probability.of.hd=logistic$fitted.values,
hd=data$hd)
# then we sort the data.frame from low probabilities to high probabilities
predicted.data <- predicted.data[
order(predicted.data$probability.of.hd, decreasing=FALSE),]
# then we add a new column to the data.frame that has the rank of each sample from low to high probability
predicted.data$rank <- 1:nrow(predicted.data)
# then we need to load the libraries
library(ggplot2)
library(cowplot)
# then we call the plot using geom_point() to draw the data
ggplot(data=predicted.data, aes(x=rank, y=probability.of.hd)) +
geom_point(aes(color=hd), alpha=1, shape=4, stroke=2) +
xlab("Index") +
ylab("Predicted probability of getting heart disease")
# and then we save the graph
ggsave("D:/GitHub/important-reference-repo/Images/heart_disease_probabilities.pdf")
