geom_freqpoly(mapping = aes(color = cut), binwidth = 500)
ggplot(data = diamonds, mapping = aes(x = price)) +
geom_histogram() +
facet_wrap(~cut, ncol = 1, scales = "free_y")
ggplot(data = diamonds, mapping = aes(x = cut, y = price)) +
geom_violin() +
coord_flip()
install.packages("ggbeeswarm")
library(ggbeeswarm)
?geom_jitter
?ggbeeswarm
?geom_quasirandom
?geom_beeswarm
ggplot(data = mpg) +
geom_quasirandom(mapping = aes(
x = reorder(class, hwy, FUN = median),
y = hwy
))
ggplot(data = mpg) +
geom_quasirandom(
mapping = aes(
x = reorder(class, hwy, FUN = median),
y = hwy
),
method = "tukey"
)
ggplot(data = mpg) +
geom_quasirandom(
mapping = aes(
x = reorder(class, hwy, FUN = median),
y = hwy
),
method = "tukeyDense"
)
ggplot(data = mpg) +
geom_quasirandom(
mapping = aes(
x = reorder(class, hwy, FUN = median),
y = hwy
),
method = "frowney"
)
ggplot(data = mpg) +
geom_quasirandom(
mapping = aes(
x = reorder(class, hwy, FUN = median),
y = hwy
),
method = "smiley"
)
ggplot(data = mpg) +
geom_beeswarm(mapping = aes(
x = reorder(class, hwy, FUN = median),
y = hwy
))
ggplot(diamonds, aes(x = cut, y = color)) +
geom_count()
diamonds %>%
count(color, cut)
diamonds %>%
count(color, cut) %>%
ggplot(aes(x = color, y = cut)) +
geom_tile(aes(fill = n))
diamonds %>%
count(color, cut) %>%
group_by(color) %>%
mutate(prop = n / sum(n)) %>%
ggplot(mapping = aes(x = color, y = cut)) +
geom_tile(mapping = aes(fill = prop))
diamonds %>%
count(color, cut) %>%
group_by(cut) %>%
mutate(prop = n / sum(n)) %>%
ggplot(mapping = aes(x = color, y = cut)) +
geom_tile(mapping = aes(fill = prop))
###
?geom_tile
flights %>%
group_by(month, dest) %>%
summarise(sep_delay = mean(dep_delay, na.rm = TRUE)) %>%
ggplot(aes(x = factor(month), y = dest, fill = dep_delay)) +
geom_tile() +
labs(x = "Month", y = "Destination", fill = "Departure Delay")
flights %>%
group_by(month, dest) %>%
summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
ggplot(aes(x = factor(month), y = dest, fill = dep_delay)) +
geom_tile() +
labs(x = "Month", y = "Destination", fill = "Departure Delay")
flights %>%
group_by(month, dest) %>%                                 # This gives us (month, dest) pairs
summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
group_by(dest) %>%                                        # group all (month, dest) pairs by dest ..
filter(n() == 12) %>%                                     # and only select those that have one entry per month
ungroup() %>%
mutate(dest = reorder(dest, dep_delay)) %>%
ggplot(aes(x = factor(month), y = dest, fill = dep_delay)) +
geom_tile() +
labs(x = "Month", y = "Destination", fill = "Departure Delay")
#> `summarise()` regrouping output by 'month' (override with `.groups` argument)
ggplot(smaller, aes(x = carat, y = price)) +
geom_point()
ggplot(diamonds, aes(x = carat, y = price)) +
geom_point()
smaller <- diamonds %>%
filter(carat <= 2.5)
ggplot(smaller, aes(x = carat, y = price)) +
geom_point()
ggplot(smaller, aes(x = carat, y = price)) +
geom_point(alpha = 1/100)
ggplot(smaller, aes(x = carat, y = price)) +
geom_bin2d()
library(hexbin)
install.packages("hexbin")
library(hexbin)
ggplot(smaller, aes(x = carat, y = price)) +
geom_hex()
ggplot(smaller, aes(x = carat, y = price)) +
geom_boxplot(aes(group = cut_width(carat, 0.1)))
# first create the dataframe which we will be working with
mouse.data <- data.frame(
weight=c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
size=c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3)
)
# you can also just load in an existing dataframe too
# first create the dataframe which we will be working with
mouse.data <- data.frame(
weight=c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
size=c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3)
)
# you can also just load in an existing dataframe too
mouse.data
# Now, we want to look at a basic plot of the data to see what it is doing
plot(mouse.data$weight, mouse.data$size)
# we can now set up the linear regression
mouse.regression <- lm(size ~ weight, data=mouse.data)
# we can now set up the linear regression
# we call the lm(linear model) function, and pass it a formula and the mouse data
# the formula is `y-values = y-intercept + slope x x-values`
# size = y-intercept + slope x weight
mouse.regression <- lm(size ~ weight, data=mouse.data)
summary(mouse.regression)
abline(mouse.regression, col="blue")
# first create the dataframe which we will be working with
mouse.data <- data.frame(
weight=c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
size=c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3)
)
# you can also just load in an existing dataframe too
mouse.data
# Now, we want to look at a basic plot of the data to see what it is doing
plot(mouse.data$weight, mouse.data$size)
# we can now set up the linear regression
# we call the lm(linear model) function, and pass it a formula and the mouse data
# the formula is `y-values = y-intercept + slope x x-values`
# size = y-intercept + slope x weight
mouse.regression <- lm(size ~ weight, data=mouse.data)
summary(mouse.regression)
# now, we can add the regression line to the graph from before
abline(mouse.regression, col="blue")
# now, we can add the regression line to the graph from before
plot(mouse.data$weight, mouse.data$size)
abline(mouse.regression, col="blue")
# first create the dataframe which we will be working with
mouse.data <- data.frame(
weight=c(0.9, 1.8, 2.4, 3.5, 3.9, 4.4, 5.1, 5.6, 6.3),
size=c(1.4, 2.6, 1.0, 3.7, 5.5, 3.2, 3.0, 4.9, 6.3),
tail=c(0.7, 1.3, 0.7, 2.0, 3.6, 3.0, 2.9, 3.9, 4.0)
)
# you can also just load in an existing dataframe too
mouse.data
plot(mouse.data$weight, mouse.data$size)
simple.regression <- lm(size ~ weight, data=mouse.data)
simple.regression <- lm(size ~ weight, data=mouse.data)
# in R, we specify the relationship we are looking for by using the `~` character
# in this example, we specify size is predicted by weight
summary(simple.regression)
plot(mouse.data$weight, mouse.data$size)
abline(simple.regression, col="red", lwd=2)
plot(mouse.data)
multiple.regression <- lm(size ~ weight + tail, data=mouse.data)
# the actual formula being used is: `size = y-intercept + slope1 x weight + slope2 x tail`
# we are specifying that size is predicted by weight and tail
summary(multiple.regression)
data.matrix <- matrix(nrow=100, ncol=10) # we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
data.matrix <- matrix(nrow=100, ncol=10) # we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
head(data.matrix)
# now that we have our fake data, our goal is to draw a plot that will show how our data is related to one another
# by default: `prcomp()` expects the samples to be rows and the genes to be columns
# since our matrix has the samples as columns and rows as genes (variables), we need to transpose the matrix using the `t()` function
# without transposing the data, we will get a graph that shows how the genes are related to one another (which is not our goal here)
pca <- prcomp(t(data.matrix), scale=TRUE)
plot(pca$x[,1], pca$x[,2]) # here we are using the first two columns in x to draw a 2-D plot that uses the first two PCs
pca.var <- pca$dev^2
# to see how much variation is in the original data each principal component (PC) accounts for, we use the square of standard deviation (`sdev`)
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
# the percentages of how much the variation accounts for is much more important than the actual number so we then calculate the percentages
pca.var <- pca$dev^2
# to see how much variation is in the original data each principal component (PC) accounts for, we use the square of standard deviation (`sdev`)
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
# the percentages of how much the variation accounts for is much more important than the actual number so we then calculate the percentages
# we can then plot these percentages with a barplot
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
data.matrix <- matrix(nrow=100, ncol=10) # we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
head(data.matrix)
# now that we have our fake data, our goal is to draw a plot that will show how our data is related to one another
# by default: `prcomp()` expects the samples to be rows and the genes to be columns
# since our matrix has the samples as columns and rows as genes (variables), we need to transpose the matrix using the `t()` function
# without transposing the data, we will get a graph that shows how the genes are related to one another (which is not our goal here)
pca <- prcomp(t(data.matrix), scale=TRUE)
plot(pca$x[,1], pca$x[,2]) # here we are using the first two columns in x to draw a 2-D plot that uses the first two PCs
# the first column is PC1 so that's on the x-axis and the second column is PC2 so that's on the y-axis
# once we plot the data, we can see there is a cluster of data on the right side and on the left side
# to get a sense of how meaningful these clusters are, we next want to see how much variation in the original data PC1 accounts for
pca.var <- pca$dev^2
# to see how much variation is in the original data each principal component (PC) accounts for, we use the square of standard deviation (`sdev`)
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
# the percentages of how much the variation accounts for is much more important than the actual number so we then calculate the percentages
# we can then plot these percentages with a barplot
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
pca.var <- pca$sdev^2
# to see how much variation is in the original data each principal component (PC) accounts for, we use the square of standard deviation (`sdev`)
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
# the percentages of how much the variation accounts for is much more important than the actual number so we then calculate the percentages
# we can then plot these percentages with a barplot
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
# we can also make a ggplot that is fancier and supplies much more information
library(ggplot2)
pca.data <- data.frame(Sample=rownames(pca$x), # one column with the sample ids
X=pca$x[,1], # these two columns for the X
Y=pca$x[,2]) # and Y coordinates for each sample
pca.data
# we can now call to the `ggplot()` function
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("My PCA Graph")
# we then want to look at the loading scores to see which genes have the largest effect on where samples are plotted in the PCA plot
# the `prcomp()` function calls the loading scores rotations
# each PC has loading scores but since PC1 accounts for 92% of the variation in the data, we will just be looking at the loading scores from PC1
# genes that push samples to the left side of the graph will have large negative values and genes that push the samples to the right side of the graph have large positive values
# since we want to look at both samples, we are going to use the `abs()` (absolute value) function to sort based on the number's magnitude rather than from high to low
gene_scores <- abs(loading_scores)
# we then want to look at the loading scores to see which genes have the largest effect on where samples are plotted in the PCA plot
# the `prcomp()` function calls the loading scores rotations
# each PC has loading scores but since PC1 accounts for 92% of the variation in the data, we will just be looking at the loading scores from PC1
loading_scores <- pca$rotation[,1]
# genes that push samples to the left side of the graph will have large negative values and genes that push the samples to the right side of the graph have large positive values
# since we want to look at both samples, we are going to use the `abs()` (absolute value) function to sort based on the number's magnitude rather than from high to low
gene_scores <- abs(loading_scores)
gene_score_ranked <- sort(gene_scores, decreasing=TRUE) # we now want to sort the magnitudes of the loading scores from high to low
# we then want to look at the loading scores to see which genes have the largest effect on where samples are plotted in the PCA plot
# the `prcomp()` function calls the loading scores rotations
# each PC has loading scores but since PC1 accounts for 92% of the variation in the data, we will just be looking at the loading scores from PC1
loading_scores <- pca$rotation[,1]
# genes that push samples to the left side of the graph will have large negative values and genes that push the samples to the right side of the graph have large positive values
# since we want to look at both samples, we are going to use the `abs()` (absolute value) function to sort based on the number's magnitude rather than from high to low
gene_scores <- abs(loading_scores)
gene_score_ranked <- sort(gene_scores, decreasing=TRUE) # we now want to sort the magnitudes of the loading scores from high to low
top_10_genes <- names(gene_score_ranked[1:10]) # we can now get the names of the top 10 genes with the largest loading score magnitudes
top_10_genes
# we can also then see which of these genes have negative loading scores (which push the "wt" samples to the left side of the graph) and which have positive loading scores (push the "wt" samples to the right side of the graph)
pca$rotation[top_10_genes,1] ## show the scores (and +/- sign)
# for a fancier looking plot, we will be using ggplot2
library(ggplot2)
data.matrix <- matrix(nrow=100, ncol=10) # we are creating the same fake data as we did in the PCA example;we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
head(data.matrix)
pca <- prcomp(t(data.matrix), scale=TRUE, center=TRUE)
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("PCA Graph")
# for a fancier looking plot, we will be using ggplot2
library(ggplot2)
data.matrix <- matrix(nrow=100, ncol=10) # we are creating the same fake data as we did in the PCA example;we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
head(data.matrix)
pca <- prcomp(t(data.matrix), scale=TRUE, center=TRUE)
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("PCA Graph")
distance.matrix <- dist(scale(t(data.matrix), center=TRUE, scale=TRUE), # just like with PCA, we need to transpose the matrix; we also center and scale the measurements for each gene which are now the columns in the data
method="euclidean") # we are also telling the `dist()` function that we want it to create the matrix using the Euclidean distance metric; this function has 6 different methods to choose from
mds.stuff <- cmdscale(distance.matrix, eig=TRUE, x.red=TRUE)
mds.stuff <- cmdscale(distance.matrix, eig=TRUE, x.ret=TRUE)
mds.var.per <- round(mds.stuff$eig/sum(mds.stuff$eig)*100, 1)
mds.data <- data.frame(Sample=rownames(mds.values),
X=mds.values[,1],
Y=mds.values[,2])
mds.values <- mds.stuff$points
mds.data <- data.frame(Sample=rownames(mds.values),
X=mds.values[,1],
Y=mds.values[,2])
ggplot(data=mds.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
theme_bw() +
xlab(paste("MDS1 - ", mds.var.per[1], "%", sep="")) +
ylab(paste("MDS2 - ", mds.var.per[2], "%", sep="")) +
ggtitle("MDS plot using Euclidean distance")
log2.data.matrix <- log2(data.matrix)
# first we create an empty matrix
log2.distance.matrix <- matrix(0,
nrow=ncol(log2.data.matrix),
ncol=ncol(log2.data.matrix),
dimnames=list(colnames(log2.data.matrix),
colnames(log2.data.matrix)))
# first we create an empty matrix
log2.distance.matrix <- matrix(0,
nrow=ncol(log2.data.matrix),
ncol=ncol(log2.data.matrix),
dimnames=list(colnames(log2.data.matrix),
colnames(log2.data.matrix)))
# then we fill the matrix with the average of the absolute values of the log fold changes
for(i in 1:ncol(log2.distance.matrix)) {
for(j in 1:i) {
log2.distance.matrix[i, j] <-
mean(abs(log2.data.matrix[,i] - log2.data.matrix[,j]))
}
}
# first we create an empty matrix
log2.distance.matrix <- matrix(0,
nrow=ncol(log2.data.matrix),
ncol=ncol(log2.data.matrix),
dimnames=list(colnames(log2.data.matrix),
colnames(log2.data.matrix)))
# then we fill the matrix with the average of the absolute values of the log fold changes
for(i in 1:ncol(log2.distance.matrix)) {
for(j in 1:i) {
log2.distance.matrix[i, j] <-
mean(abs(log2.data.matrix[,i] - log2.data.matrix[,j]))
}
}
log2.distance.matrix
# for a fancier looking plot, we will be using ggplot2
library(ggplot2)
data.matrix <- matrix(nrow=100, ncol=10) # we are creating the same fake data as we did in the PCA example;we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
head(data.matrix)
pca <- prcomp(t(data.matrix), scale=TRUE, center=TRUE)
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("PCA Graph")
distance.matrix <- dist(scale(t(data.matrix), center=TRUE, scale=TRUE), # just like with PCA, we need to transpose the matrix; we also center and scale the measurements for each gene which are now the columns in the data
method="euclidean") # we are also telling the `dist()` function that we want it to create the matrix using the Euclidean distance metric; this function has 6 different methods to choose from
mds.stuff <- cmdscale(distance.matrix, eig=TRUE, x.ret=TRUE)
mds.var.per <- round(mds.stuff$eig/sum(mds.stuff$eig)*100, 1)
mds.values <- mds.stuff$points
mds.data <- data.frame(Sample=rownames(mds.values),
X=mds.values[,1],
Y=mds.values[,2])
ggplot(data=mds.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
theme_bw() +
xlab(paste("MDS1 - ", mds.var.per[1], "%", sep="")) +
ylab(paste("MDS2 - ", mds.var.per[2], "%", sep="")) +
ggtitle("MDS plot using Euclidean distance")
log2.data.matrix <- log2(data.matrix)
# first we create an empty matrix
log2.distance.matrix <- matrix(0,
nrow=ncol(log2.data.matrix),
ncol=ncol(log2.data.matrix),
dimnames=list(colnames(log2.data.matrix),
colnames(log2.data.matrix)))
# then we fill the matrix with the average of the absolute values of the log fold changes
for(i in 1:ncol(log2.distance.matrix)) {
for(j in 1:i) {
log2.distance.matrix[i, j] <-
mean(abs(log2.data.matrix[,i] - log2.data.matrix[,j]))
}
}
log2.distance.matrix
# for a fancier looking plot, we will be using ggplot2
library(ggplot2)
data.matrix <- matrix(nrow=100, ncol=10) # we are creating the same fake data as we did in the PCA example;we are creating a matrix of data with 100 rows and 10 columns
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""), # the first 5 columns for the data will all start with "wt" for "wild type"; "wt" are normal, everyday samples
paste("ko", 1:5, sep="")) # the last 5 columns will be "ko" for "knock-out" samples; these are samples that are missing a gene because we knocked it out
rownames(data.matrix) <- paste("gene", 1:100, sep="") # this is naming each row of data (aka giving each gene a name w/ number)
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i, ] <- c(wt.values, ko.values)
} # this is giving each gene a fake read count
head(data.matrix)
pca <- prcomp(t(data.matrix), scale=TRUE, center=TRUE)
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("PCA Graph")
distance.matrix <- dist(scale(t(data.matrix), center=TRUE, scale=TRUE), # just like with PCA, we need to transpose the matrix; we also center and scale the measurements for each gene which are now the columns in the data
method="euclidean") # we are also telling the `dist()` function that we want it to create the matrix using the Euclidean distance metric; this function has 6 different methods to choose from
mds.stuff <- cmdscale(distance.matrix, eig=TRUE, x.ret=TRUE)
mds.var.per <- round(mds.stuff$eig/sum(mds.stuff$eig)*100, 1)
mds.var.per
mds.values <- mds.stuff$points
mds.data <- data.frame(Sample=rownames(mds.values),
X=mds.values[,1],
Y=mds.values[,2])
mds.data
ggplot(data=mds.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
theme_bw() +
xlab(paste("MDS1 - ", mds.var.per[1], "%", sep="")) +
ylab(paste("MDS2 - ", mds.var.per[2], "%", sep="")) +
ggtitle("MDS plot using Euclidean distance")
log2.data.matrix <- log2(data.matrix)
# first we create an empty matrix
log2.distance.matrix <- matrix(0,
nrow=ncol(log2.data.matrix),
ncol=ncol(log2.data.matrix),
dimnames=list(colnames(log2.data.matrix),
colnames(log2.data.matrix)))
# then we fill the matrix with the average of the absolute values of the log fold changes
for(i in 1:ncol(log2.distance.matrix)) {
for(j in 1:i) {
log2.distance.matrix[i, j] <-
mean(abs(log2.data.matrix[,i] - log2.data.matrix[,j]))
}
}
log2.distance.matrix
mds.stuff <- cmdscale(as.dist(log2.distance.matrix),
eig=TRUE,
x.ret=TRUE)
mds.var.per <- round(mds.stuff$eig/sum(mds.stuff$eig)*100, 1)
mds.var.per
mds.values <- mds.stuff$points
mds.data <- data.frame(Sample=rownames(mds.values),
X=mds.values[,1],
Y=mds.values[,2])
mds.data
ggplot(data=mds.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
theme_bw() +
xlab(paste("MDS1 - ", mds.var.per[1], "%", sep="")) +
ylab(paste("MDS2 - ", mds.var.per[2], "%", sep="")) +
ggtitle("MDS plot using avg(logFC) as the distance")
